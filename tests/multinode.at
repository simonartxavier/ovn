AT_BANNER([ovn multinode system tests using ovn-fake-multinode])

AT_SETUP([ovn multinode basic test])

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

# Test East-West switching
check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.0.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.0.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 1342 10.0.0.3 24 10.0.0.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 1342 10.0.0.4 24 10.0.0.1 1000::4/64 1000::a

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Add ACLs to drop all traffic
check multinode_nbctl pg-add pg0 sw0-port1 sw0-port2
check multinode_nbctl acl-add pg0 to-lport 1001 "outport == @pg0 && ip4" drop
check multinode_nbctl --wait=sb sync

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4], \
[1], [ignore])

# Add ACLs to allow icmp traffic
check multinode_nbctl acl-add pg0 to-lport 1002 "outport == @pg0 && ip4 && icmp" allow-related
check multinode_nbctl --wait=sb sync

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Create the second logical switch with one port
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 sw1-port1
check multinode_nbctl lsp-set-addresses sw1-port1 "40:54:00:00:00:03 20.0.0.3 2000::3"

# Create a logical router and attach both logical switches
check multinode_nbctl lr-add lr0
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.0.1/24 1000::a/64
check multinode_nbctl lsp-add sw0 sw0-lr0
check multinode_nbctl lsp-set-type sw0-lr0 router
check multinode_nbctl lsp-set-addresses sw0-lr0 router
check multinode_nbctl lsp-set-options sw0-lr0 router-port=lr0-sw0

check multinode_nbctl lrp-add lr0 lr0-sw1 00:00:00:00:ff:02 20.0.0.1/24 2000::a/64
check multinode_nbctl lsp-add sw1 sw1-lr0
check multinode_nbctl lsp-set-type sw1-lr0 router
check multinode_nbctl lsp-set-addresses sw1-lr0 router
check multinode_nbctl lsp-set-options sw1-lr0 router-port=lr0-sw1

m_as ovn-chassis-2 /data/create_fake_vm.sh sw1-port1 sw1p1 40:54:00:00:00:03 1342 20.0.0.3 24 20.0.0.1 2000::4/64 1000::a

m_wait_for_ports_up sw1-port1

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

check multinode_nbctl lsp-set-addresses sw1-port1 unknown
m_wait_for_ports_up sw1-port1

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

AT_CLEANUP

AT_SETUP([ovn multinode pmtu - distributed router - geneve])

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

m_as ovn-chassis-1 ip link del sw0p1-p
m_as ovn-chassis-2 ip link del sw0p2-p
m_as ovn-chassis-2 ip link del sw1p1-p

# Reset geneve tunnels
for c in ovn-chassis-1 ovn-chassis-2 ovn-gw-1
do
    m_as $c ovs-vsctl set open . external-ids:ovn-encap-type=geneve
done

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-1 ip link show | grep -q genev_sys])

# Test East-West switching
check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.0.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.0.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 1342 10.0.0.3 24 10.0.0.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 1342 10.0.0.4 24 10.0.0.1 1000::4/64 1000::a

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Create the second logical switch with one port
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 sw1-port1
check multinode_nbctl lsp-set-addresses sw1-port1 "40:54:00:00:00:03 20.0.0.3 2000::3"

# Create a logical router and attach both logical switches
check multinode_nbctl lr-add lr0
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.0.1/24 1000::a/64
check multinode_nbctl lsp-add sw0 sw0-lr0
check multinode_nbctl lsp-set-type sw0-lr0 router
check multinode_nbctl lsp-set-addresses sw0-lr0 router
check multinode_nbctl lsp-set-options sw0-lr0 router-port=lr0-sw0

check multinode_nbctl lrp-add lr0 lr0-sw1 00:00:00:00:ff:02 20.0.0.1/24 2000::a/64
check multinode_nbctl lsp-add sw1 sw1-lr0
check multinode_nbctl lsp-set-type sw1-lr0 router
check multinode_nbctl lsp-set-addresses sw1-lr0 router
check multinode_nbctl lsp-set-options sw1-lr0 router-port=lr0-sw1

m_as ovn-chassis-2 /data/create_fake_vm.sh sw1-port1 sw1p1 40:54:00:00:00:03 1342 20.0.0.3 24 20.0.0.1 2000::3/64 2000::a

# create exteranl connection for N/S traffic
check multinode_nbctl ls-add public
check multinode_nbctl lsp-add public ln-lublic
check multinode_nbctl lsp-set-type ln-lublic localnet
check multinode_nbctl lsp-set-addresses ln-lublic unknown
check multinode_nbctl lsp-set-options ln-lublic network_name=public

check multinode_nbctl lrp-add lr0 lr0-public 00:11:22:00:ff:01 172.20.0.100/24
check multinode_nbctl lsp-add public public-lr0
check multinode_nbctl lsp-set-type public-lr0 router
check multinode_nbctl lsp-set-addresses public-lr0 router
check multinode_nbctl lsp-set-options public-lr0 router-port=lr0-public
check multinode_nbctl lrp-set-gateway-chassis lr0-public ovn-gw-1 10
check multinode_nbctl lr-route-add lr0 0.0.0.0/0 172.20.0.1

check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 10.0.0.0/24
check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 20.0.0.0/24

# create some ACLs
check multinode_nbctl acl-add sw0 from-lport 1002 'ip4 || ip6'  allow-related
check multinode_nbctl acl-add sw1 from-lport 1002 'ip4 || ip6'  allow-related

# create LB
check multinode_nbctl lb-add lb0 10.0.0.1:8080 10.0.0.4:8080 udp
check multinode_nbctl ls-lb-add sw0 lb0
M_NS_DAEMONIZE([ovn-chassis-2], [sw0p2], [nc -u -l 8080 >/dev/null 2>&1], [nc.pid])

m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext0 172.20.0.1/24
m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext1 172.20.1.1/24
m_add_internal_port ovn-gw-1 ovn-ext2 br-ex ext2 172.20.1.2/24 172.20.1.1

m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex

m_wait_for_ports_up sw1-port1

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Change ptmu for the geneve tunnel
m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1200 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 5 -s 1300 -M do 20.0.0.3 2>&1 | grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1400 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping6 -c 5 -s 1450 -M do 2000::3 2>&1 | grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.1.2 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-gw-1], [ovn-ext0], [ip link set dev ext1 mtu 1000])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 10 -s 1300 -M do 172.20.1.2 2>&1 |grep -q "mtu = 1000"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.1.2 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1000 dev eth1
for i in $(seq 30); do
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [sh -c 'dd bs=512 count=2 if=/dev/urandom | nc -u 10.0.0.1 8080'], [ignore], [ignore], [ignore])
done
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route get 10.0.0.1 dev sw0p1 | grep -q 'mtu 942'])

AT_CLEANUP

AT_SETUP([ovn multinode pmtu - distributed router - vxlan])

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

m_as ovn-chassis-1 ip link del sw0p1-p
m_as ovn-chassis-2 ip link del sw0p2-p
m_as ovn-chassis-2 ip link del sw1p1-p

# Reset vxlan tunnels
for c in ovn-chassis-1 ovn-chassis-2 ovn-gw-1
do
    m_as $c ovs-vsctl set open . external-ids:ovn-encap-type=vxlan
done

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q vxlan_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q vxlan_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-1 ip link show | grep -q vxlan_sys])

# Test East-West switching
check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.0.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.0.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 1342 10.0.0.3 24 10.0.0.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 1342 10.0.0.4 24 10.0.0.1 1000::4/64 1000::a

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Create the second logical switch with one port
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 sw1-port1
check multinode_nbctl lsp-set-addresses sw1-port1 "40:54:00:00:00:03 20.0.0.3 2000::3"

# Create a logical router and attach both logical switches
check multinode_nbctl lr-add lr0
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.0.1/24 1000::a/64
check multinode_nbctl lsp-add sw0 sw0-lr0
check multinode_nbctl lsp-set-type sw0-lr0 router
check multinode_nbctl lsp-set-addresses sw0-lr0 router
check multinode_nbctl lsp-set-options sw0-lr0 router-port=lr0-sw0

check multinode_nbctl lrp-add lr0 lr0-sw1 00:00:00:00:ff:02 20.0.0.1/24 2000::a/64
check multinode_nbctl lsp-add sw1 sw1-lr0
check multinode_nbctl lsp-set-type sw1-lr0 router
check multinode_nbctl lsp-set-addresses sw1-lr0 router
check multinode_nbctl lsp-set-options sw1-lr0 router-port=lr0-sw1

m_as ovn-chassis-2 /data/create_fake_vm.sh sw1-port1 sw1p1 40:54:00:00:00:03 1342 20.0.0.3 24 20.0.0.1 2000::3/64 2000::a

# create exteranl connection for N/S traffic
check multinode_nbctl ls-add public
check multinode_nbctl lsp-add public ln-lublic
check multinode_nbctl lsp-set-type ln-lublic localnet
check multinode_nbctl lsp-set-addresses ln-lublic unknown
check multinode_nbctl lsp-set-options ln-lublic network_name=public

check multinode_nbctl lrp-add lr0 lr0-public 00:11:22:00:ff:01 172.20.0.100/24
check multinode_nbctl lsp-add public public-lr0
check multinode_nbctl lsp-set-type public-lr0 router
check multinode_nbctl lsp-set-addresses public-lr0 router
check multinode_nbctl lsp-set-options public-lr0 router-port=lr0-public
check multinode_nbctl lrp-set-gateway-chassis lr0-public ovn-gw-1 10
check multinode_nbctl lr-route-add lr0 0.0.0.0/0 172.20.0.1

check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 10.0.0.0/24
check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 20.0.0.0/24

# create some ACLs
check multinode_nbctl acl-add sw0 from-lport 1002 'ip4 || ip6'  allow-related
check multinode_nbctl acl-add sw1 from-lport 1002 'ip4 || ip6'  allow-related

m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext0 172.20.0.1/24
m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext1 172.20.1.1/24
m_add_internal_port ovn-gw-1 ovn-ext2 br-ex ext2 172.20.1.2/24 172.20.1.1

m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex

m_wait_for_ports_up sw1-port1

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Change ptmu for the vxlan tunnel
m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1200 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 5 -s 1300 -M do 20.0.0.3 2>&1 |grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

M_NS_CHECK_EXEC([ovn-gw-1], [ovn-ext0], [ip link set dev ext1 mtu 1100])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.1.2 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 20 -i 0.5 -s 1300 -M do 172.20.1.2 2>&1 |grep -q "mtu = 1150"])

AT_CLEANUP

AT_SETUP([ovn multinode pmtu - gw_router_port - geneve])

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

m_as ovn-chassis-1 ip link del sw0p1-p
m_as ovn-chassis-2 ip link del sw0p2-p
m_as ovn-chassis-2 ip link del sw1p1-p

# Reset geneve tunnels
for c in ovn-chassis-1 ovn-chassis-2 ovn-gw-1
do
    m_as $c ovs-vsctl set open . external-ids:ovn-encap-type=geneve
done

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-1 ip link show | grep -q genev_sys])

# Test East-West switching
check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.0.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.0.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 1342 10.0.0.3 24 10.0.0.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 1342 10.0.0.4 24 10.0.0.1 1000::4/64 1000::a

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Create the second logical switch with one port
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 sw1-port1
check multinode_nbctl lsp-set-addresses sw1-port1 "40:54:00:00:00:03 20.0.0.3 2000::3"

# Create a logical router and attach both logical switches
check multinode_nbctl lr-add lr0
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.0.1/24 1000::a/64
check multinode_nbctl lsp-add sw0 sw0-lr0
check multinode_nbctl lsp-set-type sw0-lr0 router
check multinode_nbctl lsp-set-addresses sw0-lr0 router
check multinode_nbctl lsp-set-options sw0-lr0 router-port=lr0-sw0

check multinode_nbctl lrp-add lr0 lr0-sw1 00:00:00:00:ff:02 20.0.0.1/24 2000::a/64
check multinode_nbctl lsp-add sw1 sw1-lr0
check multinode_nbctl lsp-set-type sw1-lr0 router
check multinode_nbctl lsp-set-addresses sw1-lr0 router
check multinode_nbctl lsp-set-options sw1-lr0 router-port=lr0-sw1

m_as ovn-chassis-2 /data/create_fake_vm.sh sw1-port1 sw1p1 40:54:00:00:00:03 1342 20.0.0.3 24 20.0.0.1 2000::3/64 2000::a

# create exteranl connection for N/S traffic
check multinode_nbctl ls-add public
check multinode_nbctl lsp-add public ln-lublic
check multinode_nbctl lsp-set-type ln-lublic localnet
check multinode_nbctl lsp-set-addresses ln-lublic unknown
check multinode_nbctl lsp-set-options ln-lublic network_name=public

check multinode_nbctl lrp-add lr0 lr0-public 00:11:22:00:ff:01 172.20.0.100/24
check multinode_nbctl lsp-add public public-lr0
check multinode_nbctl lsp-set-type public-lr0 router
check multinode_nbctl lsp-set-addresses public-lr0 router
check multinode_nbctl lsp-set-options public-lr0 router-port=lr0-public
check multinode_nbctl lrp-set-gateway-chassis lr0-public ovn-gw-1 10
check multinode_nbctl lr-route-add lr0 0.0.0.0/0 172.20.0.1

check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 10.0.0.0/24
check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 20.0.0.0/24

check multinode_nbctl lrp-set-gateway-chassis lr0-sw0 ovn-chassis-1 10
check multinode_nbctl lrp-set-gateway-chassis lr0-sw1 ovn-chassis-2 10

# create some ACLs
check multinode_nbctl acl-add sw0 from-lport 1002 'ip4 || ip6'  allow-related
check multinode_nbctl acl-add sw1 from-lport 1002 'ip4 || ip6'  allow-related

m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext0 172.20.0.1/24
m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext1 172.20.1.1/24
m_add_internal_port ovn-gw-1 ovn-ext2 br-ex ext2 172.20.1.2/24 172.20.1.1

m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex

m_wait_for_ports_up sw1-port1

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1200 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 5 -s 1300 -M do 20.0.0.3 2>&1 | grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1400 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping6 -c 5 -s 1450 -M do 2000::3 2>&1 | grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.1.2 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-gw-1], [ovn-ext0], [ip link set dev ext1 mtu 1100])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 20 -i 0.5 -s 1300 -M do 172.20.1.2 2>&1 |grep -q "mtu = 1100"])

AT_CLEANUP

AT_SETUP([ovn multinode pmtu - gw_router_port - vxlan])

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

m_as ovn-chassis-1 ip link del sw0p1-p
m_as ovn-chassis-2 ip link del sw0p2-p
m_as ovn-chassis-2 ip link del sw1p1-p

# Reset geneve tunnels
for c in ovn-chassis-1 ovn-chassis-2 ovn-gw-1
do
    m_as $c ovs-vsctl set open . external-ids:ovn-encap-type=vxlan
done

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q vxlan_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q vxlan_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-1 ip link show | grep -q vxlan_sys])

# Test East-West switching
check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.0.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.0.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 1342 10.0.0.3 24 10.0.0.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 1342 10.0.0.4 24 10.0.0.1 1000::4/64 1000::a

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Create the second logical switch with one port
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 sw1-port1
check multinode_nbctl lsp-set-addresses sw1-port1 "40:54:00:00:00:03 20.0.0.3 2000::3"

# Create a logical router and attach both logical switches
check multinode_nbctl lr-add lr0
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.0.1/24 1000::a/64
check multinode_nbctl lsp-add sw0 sw0-lr0
check multinode_nbctl lsp-set-type sw0-lr0 router
check multinode_nbctl lsp-set-addresses sw0-lr0 router
check multinode_nbctl lsp-set-options sw0-lr0 router-port=lr0-sw0

check multinode_nbctl lrp-add lr0 lr0-sw1 00:00:00:00:ff:02 20.0.0.1/24 2000::a/64
check multinode_nbctl lsp-add sw1 sw1-lr0
check multinode_nbctl lsp-set-type sw1-lr0 router
check multinode_nbctl lsp-set-addresses sw1-lr0 router
check multinode_nbctl lsp-set-options sw1-lr0 router-port=lr0-sw1

m_as ovn-chassis-2 /data/create_fake_vm.sh sw1-port1 sw1p1 40:54:00:00:00:03 1342 20.0.0.3 24 20.0.0.1 2000::3/64 2000::a

# create exteranl connection for N/S traffic
check multinode_nbctl ls-add public
check multinode_nbctl lsp-add public ln-lublic
check multinode_nbctl lsp-set-type ln-lublic localnet
check multinode_nbctl lsp-set-addresses ln-lublic unknown
check multinode_nbctl lsp-set-options ln-lublic network_name=public

check multinode_nbctl lrp-add lr0 lr0-public 00:11:22:00:ff:01 172.20.0.100/24
check multinode_nbctl lsp-add public public-lr0
check multinode_nbctl lsp-set-type public-lr0 router
check multinode_nbctl lsp-set-addresses public-lr0 router
check multinode_nbctl lsp-set-options public-lr0 router-port=lr0-public
check multinode_nbctl lrp-set-gateway-chassis lr0-public ovn-gw-1 10
check multinode_nbctl lr-route-add lr0 0.0.0.0/0 172.20.0.1

check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 10.0.0.0/24
check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 20.0.0.0/24

check multinode_nbctl lrp-set-gateway-chassis lr0-sw0 ovn-chassis-1 10
check multinode_nbctl lrp-set-gateway-chassis lr0-sw1 ovn-chassis-2 10

# create some ACLs
check multinode_nbctl acl-add sw0 from-lport 1002 'ip4 || ip6'  allow-related
check multinode_nbctl acl-add sw1 from-lport 1002 'ip4 || ip6'  allow-related

m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext0 172.20.0.1/24
m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext1 172.20.1.1/24
m_add_internal_port ovn-gw-1 ovn-ext2 br-ex ext2 172.20.1.2/24 172.20.1.1

m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex

m_wait_for_ports_up sw1-port1

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1200 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 5 -s 1300 -M do 20.0.0.3 2>&1 | grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

M_NS_CHECK_EXEC([ovn-gw-1], [ovn-ext0], [ip link set dev ext1 mtu 1100])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.1.2 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 20 -i 0.5 -s 1300 -M do 172.20.1.2 2>&1 |grep -q "mtu = 1150"])

AT_CLEANUP

AT_SETUP([ovn multinode pmtu - gw router - geneve])

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

m_as ovn-chassis-1 ip link del sw0p1-p
m_as ovn-chassis-2 ip link del sw0p2-p
m_as ovn-chassis-2 ip link del sw1p1-p

# Reset geneve tunnels
for c in ovn-chassis-1 ovn-chassis-2 ovn-gw-1
do
    m_as $c ovs-vsctl set open . external-ids:ovn-encap-type=geneve
done

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-1 ip link show | grep -q genev_sys])

# Test East-West switching
check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.0.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.0.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 1342 10.0.0.3 24 10.0.0.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 1342 10.0.0.4 24 10.0.0.1 1000::4/64 1000::a

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Create the second logical switch with one port
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 sw1-port1
check multinode_nbctl lsp-set-addresses sw1-port1 "40:54:00:00:00:03 20.0.0.3 2000::3"

# Create a logical router and attach both logical switches
check multinode_nbctl lr-add lr0 -- set Logical_Router lr0 options:chassis=ovn-gw-1
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.0.1/24 1000::a/64
check multinode_nbctl lsp-add sw0 sw0-lr0
check multinode_nbctl lsp-set-type sw0-lr0 router
check multinode_nbctl lsp-set-addresses sw0-lr0 router
check multinode_nbctl lsp-set-options sw0-lr0 router-port=lr0-sw0

check multinode_nbctl lrp-add lr0 lr0-sw1 00:00:00:00:ff:02 20.0.0.1/24 2000::a/64
check multinode_nbctl lsp-add sw1 sw1-lr0
check multinode_nbctl lsp-set-type sw1-lr0 router
check multinode_nbctl lsp-set-addresses sw1-lr0 router
check multinode_nbctl lsp-set-options sw1-lr0 router-port=lr0-sw1

m_as ovn-chassis-2 /data/create_fake_vm.sh sw1-port1 sw1p1 40:54:00:00:00:03 1342 20.0.0.3 24 20.0.0.1 2000::3/64 2000::a

# create exteranl connection for N/S traffic
check multinode_nbctl ls-add public
check multinode_nbctl lsp-add public ln-lublic
check multinode_nbctl lsp-set-type ln-lublic localnet
check multinode_nbctl lsp-set-addresses ln-lublic unknown
check multinode_nbctl lsp-set-options ln-lublic network_name=public

check multinode_nbctl lrp-add lr0 lr0-public 00:11:22:00:ff:01 172.20.0.100/24
check multinode_nbctl lsp-add public public-lr0
check multinode_nbctl lsp-set-type public-lr0 router
check multinode_nbctl lsp-set-addresses public-lr0 router
check multinode_nbctl lsp-set-options public-lr0 router-port=lr0-public
check multinode_nbctl lr-route-add lr0 0.0.0.0/0 172.20.0.1

check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 10.0.0.0/24
check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 20.0.0.0/24

# create some ACLs
check multinode_nbctl acl-add sw0 from-lport 1002 'ip4 || ip6'  allow-related
check multinode_nbctl acl-add sw1 from-lport 1002 'ip4 || ip6'  allow-related

# create LB
check multinode_nbctl lb-add lb0 10.0.0.1:8080 20.0.0.3:8080 udp
check multinode_nbctl lr-lb-add lr0 lb0
M_NS_DAEMONIZE([ovn-chassis-2], [sw1p1], [nc -u -l 8080 >/dev/null 2>&1], [nc.pid])

m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext0 172.20.0.1/24
m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext1 172.20.1.1/24
m_add_internal_port ovn-gw-1 ovn-ext2 br-ex ext2 172.20.1.2/24 172.20.1.1

m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex

m_wait_for_ports_up sw1-port1

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1200 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 5 -s 1300 -M do 20.0.0.3 2>&1 | grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1400 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping6 -c 5 -s 1450 -M do 2000::3 2>&1 | grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.1.2 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-gw-1], [ovn-ext0], [ip link set dev ext1 mtu 1100])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 20 -i 0.5 -s 1300 -M do 172.20.1.2 2>&1 | grep -q "mtu = 1100"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1000 dev eth1
for i in $(seq 30); do
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [sh -c 'dd bs=512 count=2 if=/dev/urandom | nc -u 10.0.0.1 8080'], [ignore], [ignore], [ignore])
done
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route get 10.0.0.1 dev sw0p1 | grep -q 'mtu 942'])

AT_CLEANUP

AT_SETUP([ovn multinode pmtu - gw router - vxlan])

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

m_as ovn-chassis-1 ip link del sw0p1-p
m_as ovn-chassis-2 ip link del sw0p2-p
m_as ovn-chassis-2 ip link del sw1p1-p

# Reset geneve tunnels
for c in ovn-chassis-1 ovn-chassis-2 ovn-gw-1
do
    m_as $c ovs-vsctl set open . external-ids:ovn-encap-type=vxlan
done

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q vxlan_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q vxlan_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-1 ip link show | grep -q vxlan_sys])

# Test East-West switching
check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.0.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.0.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 1342 10.0.0.3 24 10.0.0.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 1342 10.0.0.4 24 10.0.0.1 1000::4/64 1000::a

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Create the second logical switch with one port
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 sw1-port1
check multinode_nbctl lsp-set-addresses sw1-port1 "40:54:00:00:00:03 20.0.0.3 2000::3"

# Create a logical router and attach both logical switches
check multinode_nbctl lr-add lr0 -- set Logical_Router lr0 options:chassis=ovn-gw-1
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.0.1/24 1000::a/64
check multinode_nbctl lsp-add sw0 sw0-lr0
check multinode_nbctl lsp-set-type sw0-lr0 router
check multinode_nbctl lsp-set-addresses sw0-lr0 router
check multinode_nbctl lsp-set-options sw0-lr0 router-port=lr0-sw0

check multinode_nbctl lrp-add lr0 lr0-sw1 00:00:00:00:ff:02 20.0.0.1/24 2000::a/64
check multinode_nbctl lsp-add sw1 sw1-lr0
check multinode_nbctl lsp-set-type sw1-lr0 router
check multinode_nbctl lsp-set-addresses sw1-lr0 router
check multinode_nbctl lsp-set-options sw1-lr0 router-port=lr0-sw1

m_as ovn-chassis-2 /data/create_fake_vm.sh sw1-port1 sw1p1 40:54:00:00:00:03 1342 20.0.0.3 24 20.0.0.1 2000::3/64 2000::a

# create exteranl connection for N/S traffic
check multinode_nbctl ls-add public
check multinode_nbctl lsp-add public ln-lublic
check multinode_nbctl lsp-set-type ln-lublic localnet
check multinode_nbctl lsp-set-addresses ln-lublic unknown
check multinode_nbctl lsp-set-options ln-lublic network_name=public

check multinode_nbctl lrp-add lr0 lr0-public 00:11:22:00:ff:01 172.20.0.100/24
check multinode_nbctl lsp-add public public-lr0
check multinode_nbctl lsp-set-type public-lr0 router
check multinode_nbctl lsp-set-addresses public-lr0 router
check multinode_nbctl lsp-set-options public-lr0 router-port=lr0-public
check multinode_nbctl lr-route-add lr0 0.0.0.0/0 172.20.0.1

check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 10.0.0.0/24
check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 20.0.0.0/24

# create some ACLs
check multinode_nbctl acl-add sw0 from-lport 1002 'ip4 || ip6'  allow-related
check multinode_nbctl acl-add sw1 from-lport 1002 'ip4 || ip6'  allow-related

# create LB
check multinode_nbctl lb-add lb0 10.0.0.1:8080 20.0.0.3:8080 udp
check multinode_nbctl lr-lb-add lr0 lb0
M_NS_DAEMONIZE([ovn-chassis-2], [sw1p1], [nc -u -l 8080 >/dev/null 2>&1], [nc.pid])

m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext0 172.20.0.1/24
m_add_internal_port ovn-gw-1 ovn-ext0 br-ex ext1 172.20.1.1/24
m_add_internal_port ovn-gw-1 ovn-ext2 br-ex ext2 172.20.1.2/24 172.20.1.1

m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex

m_wait_for_ports_up sw1-port1

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1200 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 5 -s 1300 -M do 20.0.0.3 2>&1 | grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

M_NS_CHECK_EXEC([ovn-gw-1], [ovn-ext0], [ip link set dev ext1 mtu 1100])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.1.2 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 20 -i 0.5 -s 1300 -M do 172.20.1.2 2>&1 | grep -q "mtu = 1150"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1000 dev eth1
for i in $(seq 30); do
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [sh -c 'dd bs=512 count=2 if=/dev/urandom | nc -u 10.0.0.1 8080'], [ignore], [ignore], [ignore])
done
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route get 10.0.0.1 dev sw0p1 | grep -q 'mtu 950'])

AT_CLEANUP

m4_define([PMTUD_SWITCH_TESTS],
  [
    AT_SETUP([ovn multinode pmtu - logical switch - $1])
    encap=$1
    if test "$encap" = "vxlan"; then
      encap_sys="vxlan_sys"
      overhead=50
    else
      encap_sys="genev_sys"
      overhead=58
    fi

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

m_as ovn-chassis-1 ip link del sw0p1-p
m_as ovn-chassis-2 ip link del sw0p2-p

# Reset geneve tunnels
for c in ovn-chassis-1 ovn-chassis-2 ovn-gw-1
do
    m_as $c ovs-vsctl set open . external-ids:ovn-encap-type=$encap
done

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q $encap_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q $encap_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-1 ip link show | grep -q $encap_sys])

# Test East-West switching
check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.0.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.0.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 1342 10.0.0.3 24 10.0.0.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 1342 10.0.0.4 24 10.0.0.1 1000::4/64 1000::a

# Create the second logical switch with one port
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 sw1-port1
check multinode_nbctl lsp-set-addresses sw1-port1 "40:54:00:00:00:03 20.0.0.3 2000::3"

# Create a logical router and attach both logical switches
check multinode_nbctl lr-add lr0
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.0.1/24 1000::a/64
check multinode_nbctl lsp-add sw0 sw0-lr0
check multinode_nbctl lsp-set-type sw0-lr0 router
check multinode_nbctl lsp-set-addresses sw0-lr0 router
check multinode_nbctl lsp-set-options sw0-lr0 router-port=lr0-sw0

check multinode_nbctl lrp-add lr0 lr0-sw1 00:00:00:00:ff:02 20.0.0.1/24 2000::a/64
check multinode_nbctl lsp-add sw1 sw1-lr0
check multinode_nbctl lsp-set-type sw1-lr0 router
check multinode_nbctl lsp-set-addresses sw1-lr0 router
check multinode_nbctl lsp-set-options sw1-lr0 router-port=lr0-sw1

m_as ovn-chassis-2 /data/create_fake_vm.sh sw1-port1 sw1p1 40:54:00:00:00:03 1342 20.0.0.3 24 20.0.0.1 2000::3/64 2000::a

check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 10.0.0.0/24
check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 20.0.0.0/24

check multinode_nbctl lrp-set-gateway-chassis lr0-sw0 ovn-chassis-1 10
check multinode_nbctl lrp-set-gateway-chassis lr0-sw1 ovn-chassis-2 10

# create some ACLs
check multinode_nbctl acl-add sw0 from-lport 1002 'ip4 || ip6'  allow-related
check multinode_nbctl acl-add sw1 from-lport 1002 'ip4 || ip6'  allow-related

check multinode_nbctl lb-add lb0 10.0.0.1:8080 10.0.0.4:8080 udp
check multinode_nbctl ls-lb-add sw0 lb0
M_NS_DAEMONIZE([ovn-chassis-2], [sw0p2], [nc -u -l 8080 >/dev/null 2>&1], [nc.pid])

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Change ptmu for the geneve tunnel
m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1200 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 5 -s 1300 -M do 10.0.0.4 2>&1 | grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Change ptmu for the geneve tunnel
m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1100 dev eth1
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -c 5 -s 1300 -M do 20.0.0.3 2>&1 | grep -q -i "message too long"])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route flush dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add 10.0.0.0/24 dev sw0p1])
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route add default via 10.0.0.1 dev sw0p1])

m_as ovn-chassis-1 ip route change 170.168.0.0/16 mtu 1000 dev eth1
mtu=$((1000 - overhead))
for i in $(seq 30); do
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [sh -c 'dd bs=512 count=2 if=/dev/urandom | nc -u 10.0.0.1 8080'], [ignore], [ignore], [ignore])
done
M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ip route get 10.0.0.1 dev sw0p1 | grep -q "mtu $mtu"])

# Reset back to geneve tunnels
for c in ovn-chassis-1 ovn-chassis-2 ovn-gw-1
do
    m_as $c ovs-vsctl set open . external-ids:ovn-encap-type=geneve
done

AT_CLEANUP

AT_SETUP([ovn multinode NAT on a provider network with no localnet ports])

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

m_as ovn-chassis-1 ip link del sw0p1-p
m_as ovn-chassis-2 ip link del sw0p2-p
m_as ovn-chassis-2 ip link del sw1p1-p

check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.0.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.0.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 1342 10.0.0.3 24 10.0.0.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 1342 10.0.0.4 24 10.0.0.1 1000::4/64 1000::a

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Create the second logical switch with one port
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 sw1-port1
check multinode_nbctl lsp-set-addresses sw1-port1 "40:54:00:00:00:03 20.0.0.3 2000::3"

# Create a logical router and attach both logical switches
check multinode_nbctl lr-add lr0
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.0.1/24 1000::a/64
check multinode_nbctl lsp-add sw0 sw0-lr0
check multinode_nbctl lsp-set-type sw0-lr0 router
check multinode_nbctl lsp-set-addresses sw0-lr0 router
check multinode_nbctl lsp-set-options sw0-lr0 router-port=lr0-sw0

check multinode_nbctl lrp-add lr0 lr0-sw1 00:00:00:00:ff:02 20.0.0.1/24 2000::a/64
check multinode_nbctl lsp-add sw1 sw1-lr0
check multinode_nbctl lsp-set-type sw1-lr0 router
check multinode_nbctl lsp-set-addresses sw1-lr0 router
check multinode_nbctl lsp-set-options sw1-lr0 router-port=lr0-sw1

m_as ovn-chassis-2 /data/create_fake_vm.sh sw1-port1 sw1p1 40:54:00:00:00:03 1342 20.0.0.3 24 20.0.0.1 2000::3/64 2000::a

# create exteranl connection for N/S traffic
check multinode_nbctl ls-add public
check multinode_nbctl lsp-add public ln-public
check multinode_nbctl lsp-set-type ln-public localnet
check multinode_nbctl lsp-set-addresses ln-public unknown
check multinode_nbctl lsp-set-options ln-public network_name=public

check multinode_nbctl lrp-add lr0 lr0-public 00:11:22:00:ff:01 172.20.0.100/24
check multinode_nbctl lsp-add public public-lr0
check multinode_nbctl lsp-set-type public-lr0 router
check multinode_nbctl lsp-set-addresses public-lr0 router
check multinode_nbctl lsp-set-options public-lr0 router-port=lr0-public
check multinode_nbctl lrp-set-gateway-chassis lr0-public ovn-gw-1 10

check multinode_nbctl lr-nat-add lr0 dnat_and_snat 172.20.0.110 10.0.0.3 sw0-port1 30:54:00:00:00:03
check multinode_nbctl lr-nat-add lr0 dnat_and_snat 172.20.0.120 20.0.0.3
check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 10.0.0.0/24
check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 20.0.0.0/24

# Create a logical port pub-p1 and bind it in ovn-chassis-1
check multinode_nbctl lsp-add public public-port1
check multinode_nbctl lsp-set-addresses public-port1 "60:54:00:00:00:03 172.168.0.50"

m_as ovn-chassis-1 /data/create_fake_vm.sh public-port1 pubp1 60:54:00:00:00:03 1342 172.20.0.50 24 172.20.0.100

check multinode_nbctl --wait=hv sync

# First do basic ping tests before deleting the localnet port - ln-public.
# Once the localnet port is deleted from public ls, routing for 172.20.0.0/24
# is centralized on ovn-gw-1.

# This function checks the North-South traffic.
run_ns_traffic() {
  M_NS_CHECK_EXEC([ovn-chassis-1], [pubp1], [arp -d 172.20.0.110], [ignore], [ignore])
  M_NS_CHECK_EXEC([ovn-chassis-1], [pubp1], [arp -d 172.20.0.120], [ignore], [ignore])

  M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.0.100 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

  M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.0.110 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

  M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.0.120 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

  M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 172.20.0.50 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

  M_NS_CHECK_EXEC([ovn-chassis-2], [sw1p1], [ping -q -c 3 -i 0.3 -w 2 172.20.0.50 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

  # Now ping from pubp1 to 172.20.0.100, 172.20.0.110, 172.20.0.120, 10.0.0.3 and 20.0.0.3
  M_NS_CHECK_EXEC([ovn-chassis-1], [pubp1], [ping -q -c 3 -i 0.3 -w 2 172.20.0.100 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

  M_NS_CHECK_EXEC([ovn-chassis-1], [pubp1], [ping -q -c 3 -i 0.3 -w 2 172.20.0.110 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

  M_NS_CHECK_EXEC([ovn-chassis-1], [pubp1], [ping -q -c 3 -i 0.3 -w 2 172.20.0.120 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

  M_NS_CHECK_EXEC([ovn-chassis-1], [pubp1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

  M_NS_CHECK_EXEC([ovn-chassis-1], [pubp1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
}

# Test out the N-S traffic.
run_ns_traffic

# Delete the localnet port by changing the type of ln-public to VIF port.
check multinode_nbctl --wait=hv lsp-set-type ln-public ""

m_check_row_count Port_Binding 1 logical_port=cr-public-lr0
m_check_column chassisredirect Port_Binding type logical_port=cr-public-lr0

# Test out the N-S traffic.
run_ns_traffic

# Re-add the localnet port
check multinode_nbctl --wait=hv lsp-set-type ln-public localnet

m_check_row_count Port_Binding 0 logical_port=cr-public-lr0

# Test out the N-S traffic.
run_ns_traffic

# Delete the ln-public port this time.
check multinode_nbctl --wait=hv lsp-del ln-public

m_check_row_count Port_Binding 1 logical_port=cr-public-lr0
m_check_column chassisredirect Port_Binding type logical_port=cr-public-lr0

# Test out the N-S traffic.
run_ns_traffic

AT_CLEANUP
])

PMTUD_SWITCH_TESTS(["geneve"])
PMTUD_SWITCH_TESTS(["vxlan"])

AT_SETUP([ovn provider network - always_tunnel])

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

m_as ovn-chassis-1 ip link del sw0p1-p
m_as ovn-chassis-2 ip link del sw0p2-p
m_as ovn-chassis-2 ip link del sw1p1-p

# Reset geneve tunnels
for c in ovn-chassis-1 ovn-chassis-2 ovn-gw-1
do
    m_as $c ovs-vsctl set open . external-ids:ovn-encap-type=geneve
done

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-1 ip link show | grep -q genev_sys])

# The goal of this test case is to see the traffic works for
# E-W switching and routing when the logical switches has localnet ports
# and the option - always_tunnel=true is set.  When this option
# is set, traffic is tunneled to the destination chassis instead of using
# localnet ports.

check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.0.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.0.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 1342 10.0.0.3 24 10.0.0.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 1342 10.0.0.4 24 10.0.0.1 1000::4/64 1000::a

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Create the second logical switch with one port
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 sw1-port1
check multinode_nbctl lsp-set-addresses sw1-port1 "40:54:00:00:00:03 20.0.0.3 2000::3"

# Create a logical router and attach both logical switches
check multinode_nbctl lr-add lr0
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.0.1/24 1000::a/64
check multinode_nbctl lsp-add sw0 sw0-lr0
check multinode_nbctl lsp-set-type sw0-lr0 router
check multinode_nbctl lsp-set-addresses sw0-lr0 router
check multinode_nbctl lsp-set-options sw0-lr0 router-port=lr0-sw0

check multinode_nbctl lrp-add lr0 lr0-sw1 00:00:00:00:ff:02 20.0.0.1/24 2000::a/64
check multinode_nbctl lsp-add sw1 sw1-lr0
check multinode_nbctl lsp-set-type sw1-lr0 router
check multinode_nbctl lsp-set-addresses sw1-lr0 router
check multinode_nbctl lsp-set-options sw1-lr0 router-port=lr0-sw1

m_as ovn-chassis-2 /data/create_fake_vm.sh sw1-port1 sw1p1 40:54:00:00:00:03 1342 20.0.0.3 24 20.0.0.1 2000::3/64 2000::a

# create exteranl connection for N/S traffic
check multinode_nbctl ls-add public
check multinode_nbctl lsp-add public ln-lublic
check multinode_nbctl lsp-set-type ln-lublic localnet
check multinode_nbctl lsp-set-addresses ln-lublic unknown
check multinode_nbctl lsp-set-options ln-lublic network_name=public

check multinode_nbctl lrp-add lr0 lr0-public 00:11:22:00:ff:01 172.20.0.100/24
check multinode_nbctl lsp-add public public-lr0
check multinode_nbctl lsp-set-type public-lr0 router
check multinode_nbctl lsp-set-addresses public-lr0 router
check multinode_nbctl lsp-set-options public-lr0 router-port=lr0-public
check multinode_nbctl lrp-set-gateway-chassis lr0-public ovn-gw-1 10
check multinode_nbctl lr-route-add lr0 0.0.0.0/0 172.20.0.1

check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 10.0.0.0/24
check multinode_nbctl lr-nat-add lr0 snat 172.20.0.100 20.0.0.0/24

# create localnet ports for sw0 and sw1
check multinode_nbctl lsp-add sw0 ln-sw0
check multinode_nbctl lsp-set-type ln-sw0 localnet
check multinode_nbctl lsp-set-addresses ln-sw0 unknown
check multinode_nbctl lsp-set-options ln-sw0 network_name=public
check multinode_nbctl set logical_switch_port ln-sw0 tag_request=100

check multinode_nbctl lsp-add sw1 ln-sw1
check multinode_nbctl lsp-set-type ln-sw1 localnet
check multinode_nbctl lsp-set-addresses ln-sw1 unknown
check multinode_nbctl lsp-set-options ln-sw1 network_name=public
check multinode_nbctl set logical_switch_port ln-sw1 tag_request=101

check multinode_nbctl --wait=hv sync

M_START_TCPDUMP([ovn-chassis-1], [-c 2 -neei genev_sys_6081 icmp], [ch1_genev])
M_START_TCPDUMP([ovn-chassis-1], [-c 2 -neei eth2 icmp], [ch1_eth2])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

AT_CHECK([cat ch1_eth2.tcpdump | cut -d  ' ' -f2-22], [0], [dnl
50:54:00:00:00:03 > 50:54:00:00:00:04, ethertype 802.1Q (0x8100), length 102: vlan 100, p 0, ethertype IPv4 (0x0800), 10.0.0.3 > 10.0.0.4: ICMP echo request,
50:54:00:00:00:04 > 50:54:00:00:00:03, ethertype 802.1Q (0x8100), length 102: vlan 100, p 0, ethertype IPv4 (0x0800), 10.0.0.4 > 10.0.0.3: ICMP echo reply,
])

AT_CHECK([cat ch1_genev.tcpdump], [0], [dnl
])

m_as ovn-chassis-1 killall tcpdump
rm -f *.tcpdump
rm -f *.stderr

M_START_TCPDUMP([ovn-chassis-1], [-c 2 -neei genev_sys_6081 icmp], [ch1_genev])
M_START_TCPDUMP([ovn-chassis-1], [-c 2 -neei eth2 icmp], [ch1_eth2])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

AT_CHECK([cat ch1_eth2.tcpdump | cut -d  ' ' -f2-22], [0], [dnl
00:00:00:00:ff:02 > 40:54:00:00:00:03, ethertype 802.1Q (0x8100), length 102: vlan 101, p 0, ethertype IPv4 (0x0800), 10.0.0.3 > 20.0.0.3: ICMP echo request,
00:00:00:00:ff:01 > 50:54:00:00:00:03, ethertype 802.1Q (0x8100), length 102: vlan 100, p 0, ethertype IPv4 (0x0800), 20.0.0.3 > 10.0.0.3: ICMP echo reply,
])

AT_CHECK([cat ch1_genev.tcpdump], [0], [dnl
])

# Set the option always_tunnel=true.
# Traffic from sw0p1 to sw0p2 should be tunneled.
check multinode_nbctl set NB_Global . options:always_tunnel=true
check multinode_nbctl --wait=hv sync

m_as ovn-chassis-1 killall tcpdump
rm -f *.tcpdump
rm -f *.stderr

M_START_TCPDUMP([ovn-chassis-1], [-c 2 -neei genev_sys_6081 icmp], [ch1_genev])
M_START_TCPDUMP([ovn-chassis-1], [-c 2 -neei eth2 icmp], [ch1_eth2])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

AT_CHECK([cat ch1_genev.tcpdump | cut -d  ' ' -f2-15], [0], [dnl
50:54:00:00:00:03 > 50:54:00:00:00:04, ethertype IPv4 (0x0800), length 98: 10.0.0.3 > 10.0.0.4: ICMP echo request,
50:54:00:00:00:04 > 50:54:00:00:00:03, ethertype IPv4 (0x0800), length 98: 10.0.0.4 > 10.0.0.3: ICMP echo reply,
])

AT_CHECK([cat ch1_eth2.tcpdump], [0], [dnl
])

m_as ovn-chassis-1 killall tcpdump
rm -f *.tcpdump
rm -f *.stderr

M_START_TCPDUMP([ovn-chassis-1], [-c 2 -neei genev_sys_6081 icmp], [ch1_genev])
M_START_TCPDUMP([ovn-chassis-1], [-c 2 -neei eth2 icmp], [ch1_eth2])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

AT_CHECK([cat ch1_genev.tcpdump | cut -d  ' ' -f2-15], [0], [dnl
00:00:00:00:ff:02 > 40:54:00:00:00:03, ethertype IPv4 (0x0800), length 98: 10.0.0.3 > 20.0.0.3: ICMP echo request,
00:00:00:00:ff:01 > 50:54:00:00:00:03, ethertype IPv4 (0x0800), length 98: 20.0.0.3 > 10.0.0.3: ICMP echo reply,
])

AT_CHECK([cat ch1_eth2.tcpdump], [0], [dnl
])

m_as ovn-chassis-1 killall tcpdump
rm -f *.tcpdump
rm -f *.stderr

# Delete ln-sw1.
check multinode_nbctl --wait=hv lsp-del ln-sw1
# Traffic from sw0p1 to sw1p1 should be tunneled.

M_START_TCPDUMP([ovn-chassis-1], [-c 2 -neei genev_sys_6081 icmp], [ch1_genev])
M_START_TCPDUMP([ovn-chassis-1], [-c 2 -neei eth2 icmp], [ch1_eth2])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

AT_CHECK([cat ch1_genev.tcpdump | cut -d  ' ' -f2-15], [0], [dnl
00:00:00:00:ff:02 > 40:54:00:00:00:03, ethertype IPv4 (0x0800), length 98: 10.0.0.3 > 20.0.0.3: ICMP echo request,
00:00:00:00:ff:01 > 50:54:00:00:00:03, ethertype IPv4 (0x0800), length 98: 20.0.0.3 > 10.0.0.3: ICMP echo reply,
])

AT_CHECK([cat ch1_eth2.tcpdump], [0], [dnl
])

m_as ovn-chassis-1 killall tcpdump
rm -f *.tcpdump
rm -f *.stderr

# Make sure that traffic from sw0 still goes out of localnet port
# for IPs not managed by OVN.
# Create a fake vm in br-ex on ovn-gw-1 with IP - 10.0.0.10
m_add_internal_port ovn-gw-1 sw0-p10 br-ex sw0-p10 10.0.0.10/24
m_as ovn-gw-1 ovs-vsctl set port sw0-p10 tag=100
m_as ovn-gw-1 ip netns exec sw0-p10 ip link set sw0-p10 address 32:31:8c:da:64:4f

# Ping from sw0p1 (on ovn-chassis-1) tp sw0-p10 which is in ovn-gw-1 on
# external bridge.  The traffic path is
# sw0p1 -> br-int -> localnet port (vlan tagged 100) -> br-ex -> eth2 of ovn-chassis-1 to
# eth2 of ovn-gw-1  -> br-ex -> sw0-p10

M_START_TCPDUMP([ovn-chassis-1], [-c 2 -neei genev_sys_6081 icmp], [ch1_genev])
M_START_TCPDUMP([ovn-chassis-1], [-c 2 -neei eth2 icmp], [ch1_eth2])
M_START_TCPDUMP([ovn-gw-1], [-c 2 -neei eth2 icmp], [gw1_eth2])

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.0.10 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

m_as ovn-chassis-1 killall tcpdump
m_as ovn-gw-1 killall tcpdump

AT_CHECK([cat ch1_eth2.tcpdump | cut -d  ' ' -f2-22], [0], [dnl
50:54:00:00:00:03 > 32:31:8c:da:64:4f, ethertype 802.1Q (0x8100), length 102: vlan 100, p 0, ethertype IPv4 (0x0800), 10.0.0.3 > 10.0.0.10: ICMP echo request,
32:31:8c:da:64:4f > 50:54:00:00:00:03, ethertype 802.1Q (0x8100), length 102: vlan 100, p 0, ethertype IPv4 (0x0800), 10.0.0.10 > 10.0.0.3: ICMP echo reply,
])

AT_CHECK([cat ch1_genev.tcpdump], [0], [dnl

])

AT_CHECK([cat gw1_eth2.tcpdump | cut -d  ' ' -f2-22], [0], [dnl
50:54:00:00:00:03 > 32:31:8c:da:64:4f, ethertype 802.1Q (0x8100), length 102: vlan 100, p 0, ethertype IPv4 (0x0800), 10.0.0.3 > 10.0.0.10: ICMP echo request,
32:31:8c:da:64:4f > 50:54:00:00:00:03, ethertype 802.1Q (0x8100), length 102: vlan 100, p 0, ethertype IPv4 (0x0800), 10.0.0.10 > 10.0.0.3: ICMP echo reply,
])

rm -f *.tcpdump
rm -f *.stderr

# Add dnat_and_snat entry for 10.0.0.3 <-> 172.20.0.110
check multinode_nbctl --wait=hv lr-nat-add lr0 dnat_and_snat 172.20.0.110 10.0.0.3 sw0-port1 30:54:00:00:00:03

# Ping from sw1-p1 to 172.20.0.110
# Traffic path is
# sw1-p1 in ovn-chassis-2 -> tunnel -> ovn-gw-1 -> In ovn-gw-1 SNAT 20.0.0.3 to 172.20.0.100 ->
#  -> ln-public -> br-ex -> eth2 -> ovn-chassis-1 -> br-ex -> ln-public -> br-int ->
#  -> DNAT 172.20.0.110 to 10.0.0.3 -> sw0-p1 with src ip 172.20.0.100 and dst ip 10.0.0.3.

M_START_TCPDUMP([ovn-chassis-2], [-c 2 -neei genev_sys_6081 icmp], [ch2_genev])
M_START_TCPDUMP([ovn-chassis-2], [-c 2 -neei eth2 icmp], [ch2_eth2])
M_START_TCPDUMP([ovn-gw-1], [-c 2 -neei genev_sys_6081 icmp], [gw1_geneve])
M_START_TCPDUMP([ovn-gw-1], [-c 2 -neei eth2 icmp], [gw1_eth2])
M_START_TCPDUMP([ovn-chassis-1], [-c 2 -neei genev_sys_6081 icmp], [ch1_genev])
M_START_TCPDUMP([ovn-chassis-1], [-c 2 -neei eth2 icmp], [ch1_eth2])

M_NS_CHECK_EXEC([ovn-chassis-2], [sw1p1], [ping -q -c 3 -i 0.3 -w 2 172.20.0.110 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

m_as ovn-chassis-1 killall tcpdump
m_as ovn-chassis-2 killall tcpdump
m_as ovn-gw-1 killall tcpdump

AT_CHECK([cat ch2_genev.tcpdump | cut -d  ' ' -f2-15], [0], [dnl
00:11:22:00:ff:01 > 30:54:00:00:00:03, ethertype IPv4 (0x0800), length 98: 20.0.0.3 > 172.20.0.110: ICMP echo request,
00:00:00:00:ff:02 > 40:54:00:00:00:03, ethertype IPv4 (0x0800), length 98: 172.20.0.110 > 20.0.0.3: ICMP echo reply,
])

AT_CHECK([cat gw1_geneve.tcpdump | cut -d  ' ' -f2-15], [0], [dnl
00:11:22:00:ff:01 > 30:54:00:00:00:03, ethertype IPv4 (0x0800), length 98: 20.0.0.3 > 172.20.0.110: ICMP echo request,
00:00:00:00:ff:02 > 40:54:00:00:00:03, ethertype IPv4 (0x0800), length 98: 172.20.0.110 > 20.0.0.3: ICMP echo reply,
])

AT_CHECK([cat gw1_eth2.tcpdump | cut -d  ' ' -f2-15], [0], [dnl
00:11:22:00:ff:01 > 30:54:00:00:00:03, ethertype IPv4 (0x0800), length 98: 172.20.0.100 > 172.20.0.110: ICMP echo request,
30:54:00:00:00:03 > 00:11:22:00:ff:01, ethertype IPv4 (0x0800), length 98: 172.20.0.110 > 172.20.0.100: ICMP echo reply,
])

AT_CHECK([cat ch1_eth2.tcpdump | cut -d  ' ' -f2-15], [0], [dnl
00:11:22:00:ff:01 > 30:54:00:00:00:03, ethertype IPv4 (0x0800), length 98: 172.20.0.100 > 172.20.0.110: ICMP echo request,
30:54:00:00:00:03 > 00:11:22:00:ff:01, ethertype IPv4 (0x0800), length 98: 172.20.0.110 > 172.20.0.100: ICMP echo reply,
])

AT_CHECK([cat ch1_genev.tcpdump], [0], [dnl

])

rm -f *.tcpdump
rm -f *.stderr

# Now clear the logical_port of dnat_and_snat entry.  ovn-gw-1 should handle the DNAT.
check multinode_nbctl lr-nat-del lr0 dnat_and_snat 172.20.0.110
check multinode_nbctl --wait=hv lr-nat-add lr0 dnat_and_snat 172.20.0.110 10.0.0.3
# Ping from sw1-p1 to 172.20.0.110
# Traffic path is
# sw1-p1 in ovn-chassis-2 -> tunnel -> ovn-gw-1 -> In ovn-gw-1 SNAT 20.0.0.3 to 172.20.0.100 ->
#  DNAT 172.20.0.110 -> 10.0.0.3 -> tunnel -> ovn-chassis-1 -> br-int -> sw0p1

M_START_TCPDUMP([ovn-chassis-2], [-c 2 -neei genev_sys_6081 icmp], [ch2_genev])
M_START_TCPDUMP([ovn-chassis-2], [-c 2 -neei eth2 icmp], [ch2_eth2])
M_START_TCPDUMP([ovn-gw-1], [-c 4 -neei genev_sys_6081 icmp], [gw1_geneve])
M_START_TCPDUMP([ovn-gw-1], [-c 4 -neei eth2 icmp], [gw1_eth2])
M_START_TCPDUMP([ovn-chassis-1], [-c 2 -neei genev_sys_6081 icmp], [ch1_genev])
M_START_TCPDUMP([ovn-chassis-1], [-c 2 -neei eth2 icmp], [ch1_eth2])

M_NS_CHECK_EXEC([ovn-chassis-2], [sw1p1], [ping -q -c 3 -i 0.3 -w 2 172.20.0.110 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

m_as ovn-chassis-1 killall tcpdump
m_as ovn-chassis-2 killall tcpdump
m_as ovn-gw-1 killall tcpdump

AT_CHECK([cat ch2_genev.tcpdump | cut -d  ' ' -f2-15], [0], [dnl
00:11:22:00:ff:01 > 00:11:22:00:ff:01, ethertype IPv4 (0x0800), length 98: 20.0.0.3 > 172.20.0.110: ICMP echo request,
00:00:00:00:ff:02 > 40:54:00:00:00:03, ethertype IPv4 (0x0800), length 98: 172.20.0.110 > 20.0.0.3: ICMP echo reply,
])

AT_CHECK([cat ch1_eth2.tcpdump], [0], [dnl

])

AT_CHECK([cat gw1_geneve.tcpdump | cut -d  ' ' -f2-15], [0], [dnl
00:11:22:00:ff:01 > 00:11:22:00:ff:01, ethertype IPv4 (0x0800), length 98: 20.0.0.3 > 172.20.0.110: ICMP echo request,
00:00:00:00:ff:01 > 50:54:00:00:00:03, ethertype IPv4 (0x0800), length 98: 172.20.0.100 > 10.0.0.3: ICMP echo request,
00:11:22:00:ff:01 > 00:11:22:00:ff:01, ethertype IPv4 (0x0800), length 98: 10.0.0.3 > 172.20.0.100: ICMP echo reply,
00:00:00:00:ff:02 > 40:54:00:00:00:03, ethertype IPv4 (0x0800), length 98: 172.20.0.110 > 20.0.0.3: ICMP echo reply,
])

AT_CHECK([cat gw1_eth2.tcpdump], [0], [dnl

])

AT_CHECK([cat ch1_genev.tcpdump | cut -d  ' ' -f2-15], [0], [dnl
00:00:00:00:ff:01 > 50:54:00:00:00:03, ethertype IPv4 (0x0800), length 98: 172.20.0.100 > 10.0.0.3: ICMP echo request,
00:11:22:00:ff:01 > 00:11:22:00:ff:01, ethertype IPv4 (0x0800), length 98: 10.0.0.3 > 172.20.0.100: ICMP echo reply,
])

AT_CHECK([cat ch1_eth2.tcpdump], [0], [dnl

])

AT_CLEANUP

AT_SETUP([ovn multinode load-balancer with multiple DGPs and multiple chassis])

# Check that ovn-fake-multinode setup is up and running.
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

# Reset geneve tunnels
for c in ovn-chassis-1 ovn-chassis-2 ovn-chassis-3 ovn-chassis-4 ovn-gw-1 ovn-gw-2
do
    m_as $c ovs-vsctl set open . external-ids:ovn-encap-type=geneve
done

# Network topology
#
#             publicp1 (ovn-chassis-3) (20.0.1.3/24)
#                |
#              overlay
#                |
#      DGP public1 (ovn-gw-1) (20.0.1.1/24)
#                |
#                |
#                |
#               lr0 ------- sw0 --- sw0p1 (ovn-chassis-1) 10.0.1.3/24
#                |           |
#                |           + ---  sw0p2 (ovn-chassis-2) 10.0.1.4/24
#                |
#      DGP public2 (ovn-gw-2) (30.0.1.1/24)
#                |
#              overlay
#                |
#             publicp2 (ovn-chassis-4) (30.0.1.3/24)

# Delete already used ovs-ports
m_as ovn-chassis-1 ip link del sw0p1-p
m_as ovn-chassis-2 ip link del sw0p2-p
m_as ovn-chassis-2 ip link del sw1p1-p
m_as ovn-chassis-3 ip link del publicp1-p
m_as ovn-chassis-4 ip link del publicp2-p

# Create East-West switch for LB backends
check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.1.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.1.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 1342 10.0.1.3 24 10.0.1.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 1342 10.0.1.4 24 10.0.1.1 1000::4/64 1000::a

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.1.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-2], [sw0p2], [ping -q -c 3 -i 0.3 -w 2 10.0.1.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Create a logical router and attach to sw0
check multinode_nbctl lr-add lr0
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.1.1/24 1000::a/64
check multinode_nbctl lsp-add sw0 sw0-lr0
check multinode_nbctl lsp-set-type sw0-lr0 router
check multinode_nbctl lsp-set-addresses sw0-lr0 router
check multinode_nbctl lsp-set-options sw0-lr0 router-port=lr0-sw0

# create external connection for N/S traffic using multiple DGPs
check multinode_nbctl ls-add public

# create external connection for N/S traffic
# DGP public1
check multinode_nbctl lsp-add public ln-public-1
check multinode_nbctl lsp-set-type ln-public-1 localnet
check multinode_nbctl lsp-set-addresses ln-public-1 unknown
check multinode_nbctl lsp-set-options ln-public-1 network_name=public1

# DGP public2
check multinode_nbctl lsp-add public ln-public-2
check multinode_nbctl lsp-set-type ln-public-2 localnet
check multinode_nbctl lsp-set-addresses ln-public-2 unknown
check multinode_nbctl lsp-set-options ln-public-2 network_name=public2

# Attach DGP public1 to GW-1 and chassis-3 (overlay connectivity)
m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public1:br-ex
m_as ovn-chassis-3 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public1:br-ex

# Attach DGP public2 to GW-2 and chassis-4 (overlay connectivity)
m_as ovn-gw-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public2:br-ex
m_as ovn-chassis-4 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public2:br-ex

# Create the external LR0 port to the DGP public1
check multinode_nbctl lsp-add public public-port1
check multinode_nbctl lsp-set-addresses public-port1 "40:54:00:00:00:03 20.0.1.3 2000::3"

check multinode_nbctl lrp-add lr0 lr0-public-p1 00:00:00:00:ff:02 20.0.1.1/24 2000::a/64
check multinode_nbctl lsp-add public public-lr0-p1
check multinode_nbctl lsp-set-type public-lr0-p1 router
check multinode_nbctl lsp-set-addresses public-lr0-p1 router
check multinode_nbctl lsp-set-options public-lr0-p1 router-port=lr0-public-p1
check multinode_nbctl lrp-set-gateway-chassis lr0-public-p1 ovn-gw-1 10

# Create a VM on ovn-chassis-3 in the same public1 overlay
m_as ovn-chassis-3 /data/create_fake_vm.sh public-port1 publicp1 40:54:00:00:00:03 1342 20.0.1.3 24 20.0.1.1 2000::4/64 2000::a

m_wait_for_ports_up public-port1

# Create the external LR0 port to the DGP public2
check multinode_nbctl lsp-add public public-port2
check multinode_nbctl lsp-set-addresses public-port2 "60:54:00:00:00:03 30.0.1.3 3000::3"

check multinode_nbctl lrp-add lr0 lr0-public-p2 00:00:00:00:ff:03 30.0.1.1/24 3000::a/64
check multinode_nbctl lsp-add public public-lr0-p2
check multinode_nbctl lsp-set-type public-lr0-p2 router
check multinode_nbctl lsp-set-addresses public-lr0-p2 router
check multinode_nbctl lsp-set-options public-lr0-p2 router-port=lr0-public-p2
check multinode_nbctl lrp-set-gateway-chassis lr0-public-p2 ovn-gw-2 10

# Create a VM on ovn-chassis-4 in the same public2 overlay
m_as ovn-chassis-4 /data/create_fake_vm.sh public-port2 publicp2 60:54:00:00:00:03 1342 30.0.1.3 24 30.0.1.1 3000::4/64 3000::a

m_wait_for_ports_up public-port2

# Add SNAT rules using gateway-port
check multinode_nbctl --gateway-port lr0-public-p1 lr-nat-add lr0 snat 20.0.1.1 10.0.1.0/24
check multinode_nbctl --gateway-port lr0-public-p2 lr-nat-add lr0 snat 30.0.1.1 10.0.1.0/24

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.1.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-2], [sw0p2], [ping -q -c 3 -i 0.3 -w 2 30.0.1.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# create LB
check multinode_nbctl lb-add lb0 "172.16.0.100:80" "10.0.1.3:80,10.0.1.4:80"
check multinode_nbctl lr-lb-add lr0 lb0
check multinode_nbctl ls-lb-add sw0 lb0

# Set use_stateless_nat to true
check multinode_nbctl set load_balancer lb0 options:use_stateless_nat=true

# Start backend http services
M_NS_DAEMONIZE([ovn-chassis-1], [sw0p1], [python3 -m http.server --bind 10.0.1.3 80 >/dev/null 2>&1], [http1.pid])
M_NS_DAEMONIZE([ovn-chassis-2], [sw0p2], [python3 -m http.server --bind 10.0.1.4 80 >/dev/null 2>&1], [http2.pid])

# wait for http server be ready
OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip netns exec sw0p1 ss -tulpn | grep LISTEN | grep 10.0.1.3:80])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip netns exec sw0p2 ss -tulpn | grep LISTEN | grep 10.0.1.4:80])

# Flush conntrack entries for easier output parsing of next test.
m_as ovn-gw-1 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-2 ovs-appctl dpctl/flush-conntrack

M_NS_EXEC([ovn-chassis-3], [publicp1], [sh -c 'curl -v 172.16.0.100:80 --retry 0 --connect-timeout 1 --max-time 1 --local-port 59002 2> curl.out'])
M_NS_CHECK_EXEC([ovn-chassis-3], [publicp1], [sh -c 'cat -v curl.out' | M_FORMAT_CURL([172.16.0.100], [80])], [0], [dnl
Connected to 172.16.0.100 (172.16.0.100) port 80
200 OK
])

M_NS_EXEC([ovn-chassis-4], [publicp2], [sh -c 'curl -v 172.16.0.100:80 --retry 0 --connect-timeout 1 --max-time 1 --local-port 59003 2> curl.out'])
M_NS_CHECK_EXEC([ovn-chassis-4], [publicp2], [sh -c 'cat -v curl.out' | M_FORMAT_CURL([172.16.0.100], [80])], [0], [dnl
Connected to 172.16.0.100 (172.16.0.100) port 80
200 OK
])

m_as ovn-gw-1 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-2 ovs-appctl dpctl/flush-conntrack

M_NS_EXEC([ovn-chassis-3], [publicp1], [sh -c 'curl -v 172.16.0.100:80 --retry 0 --connect-timeout 1 --max-time 1 --local-port 59001'])
OVS_WAIT_FOR_OUTPUT([m_as ovn-gw-1 ovs-appctl dpctl/dump-conntrack | M_FORMAT_CT(20.0.1.3) | \
grep tcp | sed -E -e 's/10.0.1.3|10.0.1.4/<cleared>/g' | sort], [0], [dnl
tcp,orig=(src=20.0.1.3,dst=<cleared>,sport=59001,dport=80),reply=(src=<cleared>,dst=20.0.1.3,sport=80,dport=59001),zone=<cleared>,mark=<cleared>,protoinfo=(state=<cleared>)
tcp,orig=(src=20.0.1.3,dst=<cleared>,sport=59001,dport=80),reply=(src=<cleared>,dst=20.0.1.3,sport=80,dport=59001),zone=<cleared>,protoinfo=(state=<cleared>)
])

M_NS_EXEC([ovn-chassis-4], [publicp2], [sh -c 'curl -v 172.16.0.100:80 --retry 0 --connect-timeout 1 --max-time 1 --local-port 59000'])
OVS_WAIT_FOR_OUTPUT([m_as ovn-gw-2 ovs-appctl dpctl/dump-conntrack | M_FORMAT_CT(30.0.1.3) | \
grep tcp | sed -E -e 's/10.0.1.3|10.0.1.4/<cleared>/g' | sort], [0], [dnl
tcp,orig=(src=30.0.1.3,dst=<cleared>,sport=59000,dport=80),reply=(src=<cleared>,dst=30.0.1.3,sport=80,dport=59000),zone=<cleared>,mark=<cleared>,protoinfo=(state=<cleared>)
tcp,orig=(src=30.0.1.3,dst=<cleared>,sport=59000,dport=80),reply=(src=<cleared>,dst=30.0.1.3,sport=80,dport=59000),zone=<cleared>,protoinfo=(state=<cleared>)
])

# create a big file on web servers for download
M_NS_EXEC([ovn-chassis-1], [sw0p1], [dd bs=512 count=200000 if=/dev/urandom of=download_file])
M_NS_EXEC([ovn-chassis-2], [sw0p2], [dd bs=512 count=200000 if=/dev/urandom of=download_file])

# Flush conntrack entries for easier output parsing of next test.
m_as ovn-chassis-1 ovs-appctl dpctl/flush-conntrack
m_as ovn-chassis-2 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-1 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-2 ovs-appctl dpctl/flush-conntrack

M_NS_EXEC([ovn-chassis-3], [publicp1], [sh -c 'curl -v -O 172.16.0.100:80/download_file --retry 0 --connect-timeout 1 --max-time 1 --local-port 59004 2>curl.out'])

gw1_ct=$(m_as ovn-gw-1 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
gw2_ct=$(m_as ovn-gw-2 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
chassis1_ct=$(m_as ovn-chassis-1 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
chassis2_ct=$(m_as ovn-chassis-2 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
chassis1_flow=$(m_as ovn-chassis-1 ovs-dpctl dump-flows | sed ':a;N;$!ba;s/\n/\\n/g')
chassis2_flow=$(m_as ovn-chassis-2 ovs-dpctl dump-flows | sed ':a;N;$!ba;s/\n/\\n/g')

OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 ip netns exec publicp1 cat -v curl.out | M_FORMAT_CURL([172.16.0.100], [80])], [0], [dnl
Connected to 172.16.0.100 (172.16.0.100) port 80
200 OK
])

# Check if we have only one backend for the same connection - orig + dest ports
OVS_WAIT_FOR_OUTPUT([echo -e $gw1_ct | M_FORMAT_CT(20.0.1.3) | \
grep tcp | sed -E -e 's/10.0.1.3|10.0.1.4/<cleared>/g' | sort], [0], [dnl
tcp,orig=(src=20.0.1.3,dst=<cleared>,sport=59004,dport=80),reply=(src=<cleared>,dst=20.0.1.3,sport=80,dport=59004),zone=<cleared>,mark=<cleared>,protoinfo=(state=<cleared>)
tcp,orig=(src=20.0.1.3,dst=<cleared>,sport=59004,dport=80),reply=(src=<cleared>,dst=20.0.1.3,sport=80,dport=59004),zone=<cleared>,protoinfo=(state=<cleared>)
])

# Check if gw-2 is empty to ensure that the traffic only come from/to the originator chassis via DGP public1
AT_CHECK([echo -e $gw2_ct | grep "20.0.1.3" -c], [1], [dnl
0
])

# Check the backend IP from ct entries on gw-1 (DGP public1)
backend_check=$(echo -e $chassis1_ct | grep "10.0.1.3,sport=59004,dport=80" -c)

if [[ $backend_check -gt 0 ]]; then
# Backend resides on ovn-chassis-1
AT_CHECK([echo -e $chassis1_ct | M_FORMAT_CT(20.0.1.3) | \
grep tcp], [0], [dnl
tcp,orig=(src=20.0.1.3,dst=10.0.1.3,sport=59004,dport=80),reply=(src=10.0.1.3,dst=20.0.1.3,sport=80,dport=59004),zone=<cleared>,protoinfo=(state=<cleared>)
])

# Ensure that the traffic only come from ovn-chassis-1
AT_CHECK([echo -e $chassis2_ct | grep "20.0.1.3" | grep "dport=80" -c], [1], [dnl
0
])
AT_CHECK([echo -e $chassis2_flow | grep "20.0.1.3" | grep "dport=80" -c], [1], [dnl
0
])
else
# Backend resides on ovn-chassis-2
AT_CHECK([echo -e $chassis2_ct | M_FORMAT_CT(20.0.1.3) | \
grep tcp], [0], [dnl
tcp,orig=(src=20.0.1.3,dst=10.0.1.4,sport=59004,dport=80),reply=(src=10.0.1.4,dst=20.0.1.3,sport=80,dport=59004),zone=<cleared>,protoinfo=(state=<cleared>)
])

# Ensure that the traffic only come from ovn-chassis-2
AT_CHECK([echo -e $chassis1_ct | grep "20.0.1.3" | grep "dport=80" -c], [1], [dnl
0
])
AT_CHECK([echo -e $chassis1_flow | grep "20.0.1.3" | grep "dport=80" -c], [1], [dnl
0
])
fi

# Flush conntrack entries for easier output parsing of next test.
m_as ovn-chassis-1 ovs-appctl dpctl/flush-conntrack
m_as ovn-chassis-2 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-1 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-2 ovs-appctl dpctl/flush-conntrack

# Check the flows again for a new source port
M_NS_EXEC([ovn-chassis-3], [publicp1], [sh -c 'curl -v -O 172.16.0.100:80/download_file --retry 0 --connect-timeout 1 --max-time 1 --local-port 59005 2>curl.out'])

gw1_ct=$(m_as ovn-gw-1 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
gw2_ct=$(m_as ovn-gw-2 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
chassis1_ct=$(m_as ovn-chassis-1 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
chassis2_ct=$(m_as ovn-chassis-2 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
chassis1_flow=$(m_as ovn-chassis-1 ovs-dpctl dump-flows | sed ':a;N;$!ba;s/\n/\\n/g')
chassis2_flow=$(m_as ovn-chassis-2 ovs-dpctl dump-flows | sed ':a;N;$!ba;s/\n/\\n/g')

OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 ip netns exec publicp1 cat -v curl.out | M_FORMAT_CURL([172.16.0.100], [80])], [0], [dnl
Connected to 172.16.0.100 (172.16.0.100) port 80
200 OK
])

# Check if we have only one backend for the same connection - orig + dest ports
OVS_WAIT_FOR_OUTPUT([echo -e $gw1_ct | M_FORMAT_CT(20.0.1.3) | \
grep tcp | sed -E -e 's/10.0.1.3|10.0.1.4/<cleared>/g' | sort], [0], [dnl
tcp,orig=(src=20.0.1.3,dst=<cleared>,sport=59005,dport=80),reply=(src=<cleared>,dst=20.0.1.3,sport=80,dport=59005),zone=<cleared>,mark=<cleared>,protoinfo=(state=<cleared>)
tcp,orig=(src=20.0.1.3,dst=<cleared>,sport=59005,dport=80),reply=(src=<cleared>,dst=20.0.1.3,sport=80,dport=59005),zone=<cleared>,protoinfo=(state=<cleared>)
])

# Check if gw-2 is empty to ensure that the traffic only come from/to the originator chassis via DGP public1
AT_CHECK([echo -e $gw2_ct | grep "20.0.1.3" -c], [1], [dnl
0
])

# Check the backend IP from ct entries on gw-1 (DGP public1)
backend_check=$(echo -e $chassis1_ct | grep "10.0.1.3,sport=59005,dport=80" -c)

if [[ $backend_check -gt 0 ]]; then
# Backend resides on ovn-chassis-1
# Ensure that the traffic only come from ovn-chassis-1
AT_CHECK([echo -e $chassis2_ct | grep "20.0.1.3" | grep "dport=80" -c], [1], [dnl
0
])
AT_CHECK([echo -e $chassis2_flow | grep "20.0.1.3" | grep "dport=80" -c], [1], [dnl
0
])
else
# Backend resides on ovn-chassis-2
# Ensure that the traffic only come from ovn-chassis-2
AT_CHECK([echo -e $chassis1_ct | grep "20.0.1.3" | grep "dport=80" -c], [1], [dnl
0
])
AT_CHECK([echo -e $chassis1_flow | grep "20.0.1.3" | grep "dport=80" -c], [1], [dnl
0
])
fi

# Flush conntrack entries for easier output parsing of next test.
m_as ovn-chassis-1 ovs-appctl dpctl/flush-conntrack
m_as ovn-chassis-2 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-1 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-2 ovs-appctl dpctl/flush-conntrack

# Start a new test using the second DGP as origin (public2)
M_NS_EXEC([ovn-chassis-4], [publicp2], [sh -c 'curl -v -O 172.16.0.100:80/download_file --retry 0 --connect-timeout 1 --max-time 1 --local-port 59006 2>curl.out'])

gw1_ct=$(m_as ovn-gw-1 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
gw2_ct=$(m_as ovn-gw-2 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
chassis1_ct=$(m_as ovn-chassis-1 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
chassis2_ct=$(m_as ovn-chassis-2 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
chassis1_flow=$(m_as ovn-chassis-1 ovs-dpctl dump-flows | sed ':a;N;$!ba;s/\n/\\n/g')
chassis2_flow=$(m_as ovn-chassis-2 ovs-dpctl dump-flows | sed ':a;N;$!ba;s/\n/\\n/g')

OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-4 ip netns exec publicp2 cat -v curl.out | M_FORMAT_CURL([172.16.0.100], [80])], [0], [dnl
Connected to 172.16.0.100 (172.16.0.100) port 80
200 OK
])

# Check if we have only one backend for the same connection - orig + dest ports
OVS_WAIT_FOR_OUTPUT([echo -e $gw2_ct | M_FORMAT_CT(30.0.1.3) | \
grep tcp | sed -E -e 's/10.0.1.3|10.0.1.4/<cleared>/g' | sort], [0], [dnl
tcp,orig=(src=30.0.1.3,dst=<cleared>,sport=59006,dport=80),reply=(src=<cleared>,dst=30.0.1.3,sport=80,dport=59006),zone=<cleared>,mark=<cleared>,protoinfo=(state=<cleared>)
tcp,orig=(src=30.0.1.3,dst=<cleared>,sport=59006,dport=80),reply=(src=<cleared>,dst=30.0.1.3,sport=80,dport=59006),zone=<cleared>,protoinfo=(state=<cleared>)
])

# Check if gw-1 is empty to ensure that the traffic only come from/to the originator chassis via DGP public2
AT_CHECK([echo -e $gw1_ct | grep "30.0.1.3" -c], [1], [dnl
0
])

# Check the backend IP from ct entries on gw-2 (DGP public2)
backend_check=$(echo -e $chassis1_ct | grep "10.0.1.3,sport=59006,dport=80" -c)

if [[ $backend_check -gt 0 ]]; then
# Backend resides on ovn-chassis-1
AT_CHECK([echo -e $chassis1_ct | M_FORMAT_CT(30.0.1.3) | \
grep tcp], [0], [dnl
tcp,orig=(src=30.0.1.3,dst=10.0.1.3,sport=59006,dport=80),reply=(src=10.0.1.3,dst=30.0.1.3,sport=80,dport=59006),zone=<cleared>,protoinfo=(state=<cleared>)
])

# Ensure that the traffic only come from ovn-chassis-1
AT_CHECK([echo -e $chassis2_ct | grep "30.0.1.3" | grep "dport=80" -c], [1], [dnl
0
])
AT_CHECK([echo -e $chassis2_flow | grep "30.0.1.3" | grep "dport=80" -c], [1], [dnl
0
])
else
# Backend resides on ovn-chassis-2
AT_CHECK([echo -e $chassis2_ct | M_FORMAT_CT(30.0.1.3) | \
grep tcp], [0], [dnl
tcp,orig=(src=30.0.1.3,dst=10.0.1.4,sport=59006,dport=80),reply=(src=10.0.1.4,dst=30.0.1.3,sport=80,dport=59006),zone=<cleared>,protoinfo=(state=<cleared>)
])

# Ensure that the traffic only come from ovn-chassis-2
AT_CHECK([echo -e $chassis1_ct | grep "30.0.1.3" | grep "dport=80" -c], [1], [dnl
0
])
AT_CHECK([echo -e $chassis1_flow | grep "30.0.1.3" | grep "dport=80" -c], [1], [dnl
0
])
fi

# Flush conntrack entries for easier output parsing of next test.
m_as ovn-chassis-1 ovs-appctl dpctl/flush-conntrack
m_as ovn-chassis-2 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-1 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-2 ovs-appctl dpctl/flush-conntrack

# Check the flows again for a new source port using the second DGP as origin (public2)
M_NS_EXEC([ovn-chassis-4], [publicp2], [sh -c 'curl -v -O 172.16.0.100:80/download_file --retry 0 --connect-timeout 1 --max-time 1 --local-port 59007 2>curl.out'])

gw1_ct=$(m_as ovn-gw-1 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
gw2_ct=$(m_as ovn-gw-2 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
chassis1_ct=$(m_as ovn-chassis-1 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
chassis2_ct=$(m_as ovn-chassis-2 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
chassis1_flow=$(m_as ovn-chassis-1 ovs-dpctl dump-flows | sed ':a;N;$!ba;s/\n/\\n/g')
chassis2_flow=$(m_as ovn-chassis-2 ovs-dpctl dump-flows | sed ':a;N;$!ba;s/\n/\\n/g')

OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-4 ip netns exec publicp2 cat -v curl.out | M_FORMAT_CURL([172.16.0.100], [80])], [0], [dnl
Connected to 172.16.0.100 (172.16.0.100) port 80
200 OK
])

# Check if we have only one backend for the same connection - orig + dest ports
OVS_WAIT_FOR_OUTPUT([echo -e $gw2_ct | M_FORMAT_CT(30.0.1.3) | \
grep tcp | sed -E -e 's/10.0.1.3|10.0.1.4/<cleared>/g' | sort], [0], [dnl
tcp,orig=(src=30.0.1.3,dst=<cleared>,sport=59007,dport=80),reply=(src=<cleared>,dst=30.0.1.3,sport=80,dport=59007),zone=<cleared>,mark=<cleared>,protoinfo=(state=<cleared>)
tcp,orig=(src=30.0.1.3,dst=<cleared>,sport=59007,dport=80),reply=(src=<cleared>,dst=30.0.1.3,sport=80,dport=59007),zone=<cleared>,protoinfo=(state=<cleared>)
])

# Check if gw-1 is empty to ensure that the traffic only come from/to the originator chassis via DGP public2
AT_CHECK([echo -e $gw1_ct | grep "30.0.1.3" -c], [1], [dnl
0
])

# Check the backend IP from ct entries on gw-1 (DGP public1)
backend_check=$(echo -e $chassis1_ct | grep "10.0.1.3,sport=59007,dport=80" -c)

if [[ $backend_check -gt 0 ]]; then
# Backend resides on ovn-chassis-1
AT_CHECK([echo -e $chassis1_ct | M_FORMAT_CT(30.0.1.3) | \
grep tcp], [0], [dnl
tcp,orig=(src=30.0.1.3,dst=10.0.1.3,sport=59007,dport=80),reply=(src=10.0.1.3,dst=30.0.1.3,sport=80,dport=59007),zone=<cleared>,protoinfo=(state=<cleared>)
])

# Ensure that the traffic only come from ovn-chassis-1
AT_CHECK([echo -e $chassis2_ct | grep "30.0.1.3" | grep "dport=80" -c], [1], [dnl
0
])
AT_CHECK([echo -e $chassis2_flow | grep "30.0.1.3" | grep "dport=80" -c], [1], [dnl
0
])
else
# Backend resides on ovn-chassis-2
AT_CHECK([echo -e $chassis2_ct | M_FORMAT_CT(30.0.1.3) | \
grep tcp], [0], [dnl
tcp,orig=(src=30.0.1.3,dst=10.0.1.4,sport=59007,dport=80),reply=(src=10.0.1.4,dst=30.0.1.3,sport=80,dport=59007),zone=<cleared>,protoinfo=(state=<cleared>)
])

# Ensure that the traffic only come from ovn-chassis-2
AT_CHECK([echo -e $chassis1_ct | grep "30.0.1.3" | grep "dport=80" -c], [1], [dnl
0
])
AT_CHECK([echo -e $chassis1_flow | grep "30.0.1.3" | grep "dport=80" -c], [1], [dnl
0
])
fi

# Check multiple requests coming from DGP's public1 and public2

M_NS_EXEC([ovn-chassis-4], [publicp2], [sh -c 'curl -v -O 172.16.0.100:80/download_file --retry 0 --connect-timeout 1 --max-time 1 2>curl.out'])
OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-4 cat -v curl.out | M_FORMAT_CURL([172.16.0.100], [80])], [0], [dnl
Connected to 172.16.0.100 (172.16.0.100) port 80
200 OK
])

M_NS_EXEC([ovn-chassis-3], [publicp1], [sh -c 'curl -v -O 172.16.0.100:80/download_file --retry 0 --connect-timeout 1 --max-time 1 2>curl.out'])
OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 cat -v curl.out | M_FORMAT_CURL([172.16.0.100], [80])], [0], [dnl
Connected to 172.16.0.100 (172.16.0.100) port 80
200 OK
])

M_NS_EXEC([ovn-chassis-4], [publicp2], [sh -c 'curl -v -O 172.16.0.100:80/download_file --retry 0 --connect-timeout 1 --max-time 1 2>curl.out'])
OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-4 cat -v curl.out | M_FORMAT_CURL([172.16.0.100], [80])], [0], [dnl
Connected to 172.16.0.100 (172.16.0.100) port 80
200 OK
])

M_NS_EXEC([ovn-chassis-3], [publicp1], [sh -c 'curl -v -O 172.16.0.100:80/download_file --retry 0 --connect-timeout 1 --max-time 1 2>curl.out'])
OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 cat -v curl.out | M_FORMAT_CURL([172.16.0.100], [80])], [0], [dnl
Connected to 172.16.0.100 (172.16.0.100) port 80
200 OK
])

# Remove the LB and change the VIP port - different from the backend ports
check multinode_nbctl lb-del lb0

# create LB again
check multinode_nbctl lb-add lb0 "172.16.0.100:9000" "10.0.1.3:80,10.0.1.4:80"
check multinode_nbctl lr-lb-add lr0 lb0
check multinode_nbctl ls-lb-add sw0 lb0

# Set use_stateless_nat to true
check multinode_nbctl set load_balancer lb0 options:use_stateless_nat=true

m_as ovn-gw-1 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-2 ovs-appctl dpctl/flush-conntrack

# Check end-to-end request using a new port for VIP
M_NS_EXEC([ovn-chassis-3], [publicp1], [sh -c 'curl -v -O 172.16.0.100:9000/download_file --retry 0 --connect-timeout 1 --max-time 1 --local-port 59008 2>curl.out'])
OVS_WAIT_FOR_OUTPUT([m_as ovn-gw-1 ovs-appctl dpctl/dump-conntrack | M_FORMAT_CT(20.0.1.3) | \
grep tcp | sed -E -e 's/10.0.1.3|10.0.1.4/<cleared>/g' | sort], [0], [dnl
tcp,orig=(src=20.0.1.3,dst=<cleared>,sport=59008,dport=80),reply=(src=<cleared>,dst=20.0.1.3,sport=80,dport=59008),zone=<cleared>,protoinfo=(state=<cleared>)
tcp,orig=(src=20.0.1.3,dst=<cleared>,sport=59008,dport=9000),reply=(src=<cleared>,dst=20.0.1.3,sport=80,dport=59008),zone=<cleared>,mark=<cleared>,protoinfo=(state=<cleared>)
])

OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 cat -v curl.out | M_FORMAT_CURL([172.16.0.100], [9000])], [0], [dnl
Connected to 172.16.0.100 (172.16.0.100) port 9000
200 OK
])

m_as ovn-gw-1 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-2 ovs-appctl dpctl/flush-conntrack

# Check end-to-end request using a new port for VIP
M_NS_EXEC([ovn-chassis-4], [publicp2], [sh -c 'curl -v -O 172.16.0.100:9000/download_file --retry 0 --connect-timeout 1 --max-time 1 --local-port 59008 2>curl.out'])
OVS_WAIT_FOR_OUTPUT([m_as ovn-gw-2 ovs-appctl dpctl/dump-conntrack | M_FORMAT_CT(30.0.1.3) | \
grep tcp | sed -E -e 's/10.0.1.3|10.0.1.4/<cleared>/g' | sort], [0], [dnl
tcp,orig=(src=30.0.1.3,dst=<cleared>,sport=59008,dport=80),reply=(src=<cleared>,dst=30.0.1.3,sport=80,dport=59008),zone=<cleared>,protoinfo=(state=<cleared>)
tcp,orig=(src=30.0.1.3,dst=<cleared>,sport=59008,dport=9000),reply=(src=<cleared>,dst=30.0.1.3,sport=80,dport=59008),zone=<cleared>,mark=<cleared>,protoinfo=(state=<cleared>)
])

OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 cat -v curl.out | M_FORMAT_CURL([172.16.0.100], [9000])], [0], [dnl
Connected to 172.16.0.100 (172.16.0.100) port 9000
200 OK
])

AT_CLEANUP

AT_SETUP([ovn multinode load-balancer with multiple DGPs and multiple chassis - ECMP environment])

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

# Reset geneve tunnels
for c in ovn-chassis-1 ovn-chassis-2 ovn-chassis-3 ovn-gw-1 ovn-gw-2 ovn-gw-3 ovn-gw-4
do
    m_as $c ovs-vsctl set open . external-ids:ovn-encap-type=geneve
done

# Network topology
#                                      VM ovn-chassis-3 (40.0.2.3/24)
#                                                   |
#                                                  sw1
#                                                   |
#                                                  lr1
#                                                   |
#                     +.............................|.............................+
#                     |                                                           |
#      DGP publicp3 (ovn-gw-3) (20.0.2.3/24)                     DGP publicp4 (ovn-gw-4) (20.0.2.4/24)
#                     |                                                           |
#                     +.............................+.............................+
#                                                   |
#                                                   | (overlay)
#                     +.............................+.............................+
#                     |                                                           |
#      DGP public1 (ovn-gw-1) (20.0.2.1/24)                      DGP public2 (ovn-gw-2) (20.0.2.2/24)
#                     |                                                           |
#                     +.............................+.............................+
#                                                   |
#                                                  lr0 (lb0 VIP 172.16.0.100)
#                                                   |
#                                                  sw0
#                                                   |
#                     +.............................+.............................+
#                     |                                                           |
#      sw0p1 (ovn-chassis-1) 10.0.2.3/24                         sw0p2 (ovn-chassis-2) 10.0.2.4/24


# Delete already used ovs-ports
m_as ovn-chassis-1 ip link del sw0p1-p
m_as ovn-chassis-2 ip link del sw0p2-p
m_as ovn-chassis-2 ip link del sw1p1-p
m_as ovn-chassis-3 ip link del sw1p1-p

# Create East-West switch for LB backends
check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 sw0-port1
check multinode_nbctl lsp-set-addresses sw0-port1 "50:54:00:00:00:03 10.0.2.3 1000::3"
check multinode_nbctl lsp-add sw0 sw0-port2
check multinode_nbctl lsp-set-addresses sw0-port2 "50:54:00:00:00:04 10.0.2.4 1000::4"

m_as ovn-chassis-1 /data/create_fake_vm.sh sw0-port1 sw0p1 50:54:00:00:00:03 1342 10.0.2.3 24 10.0.2.1 1000::3/64 1000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh sw0-port2 sw0p2 50:54:00:00:00:04 1342 10.0.2.4 24 10.0.2.1 1000::4/64 1000::a

# Create sw1 for ovn-chassis-3 VM
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 sw1-port1
check multinode_nbctl lsp-set-addresses sw1-port1 "70:54:00:00:00:03 40.0.2.3 5000::3"

m_as ovn-chassis-3 /data/create_fake_vm.sh sw1-port1 sw1p1 70:54:00:00:00:03 1342 40.0.2.3 24 40.0.2.1 5000::3/64 5000::a

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 10.0.2.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-2], [sw0p2], [ping -q -c 3 -i 0.3 -w 2 10.0.2.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Create a logical router and attach to sw0
check multinode_nbctl lr-add lr0
check multinode_nbctl lrp-add lr0 lr0-sw0 00:00:00:00:ff:01 10.0.2.1/24 1000::a/64
check multinode_nbctl lsp-add sw0 sw0-lr0
check multinode_nbctl lsp-set-type sw0-lr0 router
check multinode_nbctl lsp-set-addresses sw0-lr0 router
check multinode_nbctl lsp-set-options sw0-lr0 router-port=lr0-sw0

# create external connection for N/S traffic using multiple DGPs
check multinode_nbctl ls-add public

# create external connection for N/S traffic
# DGP public1
check multinode_nbctl lsp-add public ln-public-1
check multinode_nbctl lsp-set-type ln-public-1 localnet
check multinode_nbctl lsp-set-addresses ln-public-1 unknown
check multinode_nbctl lsp-set-options ln-public-1 network_name=public1

# DGP public2
check multinode_nbctl lsp-add public ln-public-2
check multinode_nbctl lsp-set-type ln-public-2 localnet
check multinode_nbctl lsp-set-addresses ln-public-2 unknown
check multinode_nbctl lsp-set-options ln-public-2 network_name=public2

# Attach DGP public1 to GW-1 public1 (overlay connectivity)
m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public1:br-ex

# Attach DGP public2 to GW-2 public2 (overlay connectivity)
m_as ovn-gw-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public2:br-ex

check multinode_nbctl lrp-add lr0 lr0-public-p1 40:54:00:00:00:01 20.0.2.1/24 2000::1/64
check multinode_nbctl lsp-add public public-lr0-p1
check multinode_nbctl lsp-set-type public-lr0-p1 router
check multinode_nbctl lsp-set-addresses public-lr0-p1 router
check multinode_nbctl lsp-set-options public-lr0-p1 router-port=lr0-public-p1
check multinode_nbctl lrp-set-gateway-chassis lr0-public-p1 ovn-gw-1 10

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [sw0p1], [ping -q -c 3 -i 0.3 -w 2 20.0.2.1 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

check multinode_nbctl lrp-add lr0 lr0-public-p2 40:54:00:00:00:02 20.0.2.2/24 2000::2/64
check multinode_nbctl lsp-add public public-lr0-p2
check multinode_nbctl lsp-set-type public-lr0-p2 router
check multinode_nbctl lsp-set-addresses public-lr0-p2 router
check multinode_nbctl lsp-set-options public-lr0-p2 router-port=lr0-public-p2
check multinode_nbctl lrp-set-gateway-chassis lr0-public-p2 ovn-gw-2 10

M_NS_CHECK_EXEC([ovn-chassis-2], [sw0p2], [ping -q -c 3 -i 0.3 -w 2 20.0.2.2 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Create a logical router and attach to sw1
check multinode_nbctl lr-add lr1
check multinode_nbctl lrp-add lr1 lr1-sw1 00:00:00:00:ff:02 40.0.2.1/24 5000::a/64
check multinode_nbctl lsp-add sw1 sw1-lr1
check multinode_nbctl lsp-set-type sw1-lr1 router
check multinode_nbctl lsp-set-addresses sw1-lr1 router
check multinode_nbctl lsp-set-options sw1-lr1 router-port=lr1-sw1

# create external connection for N/S traffic
# DGP public3
check multinode_nbctl lsp-add public ln-public-3
check multinode_nbctl lsp-set-type ln-public-3 localnet
check multinode_nbctl lsp-set-addresses ln-public-3 unknown
check multinode_nbctl lsp-set-options ln-public-3 network_name=public3

# Attach DGP public3 to GW-3 public3 (overlay connectivity)
m_as ovn-gw-3 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public3:br-ex

check multinode_nbctl lrp-add lr1 lr1-public-p3 40:54:00:00:00:03 20.0.2.3/24 2000::3/64
check multinode_nbctl lsp-add public public-lr1-p3
check multinode_nbctl lsp-set-type public-lr1-p3 router
check multinode_nbctl lsp-set-addresses public-lr1-p3 router
check multinode_nbctl lsp-set-options public-lr1-p3 router-port=lr1-public-p3
check multinode_nbctl lrp-set-gateway-chassis lr1-public-p3 ovn-gw-3 10

M_NS_CHECK_EXEC([ovn-chassis-3], [sw1p1], [ping -q -c 3 -i 0.3 -w 2 40.0.2.1 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-3], [sw1p1], [ping -q -c 3 -i 0.3 -w 2 20.0.2.3 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Add a default route for multiple DGPs using ECMP - first step
check multinode_nbctl --ecmp lr-route-add lr0 0.0.0.0/0 20.0.2.3
check multinode_nbctl --ecmp lr-route-add lr1 0.0.0.0/0 20.0.2.1

# Add SNAT rules using gateway-port
check multinode_nbctl --gateway-port lr0-public-p1 lr-nat-add lr0 snat 20.0.2.1 10.0.2.0/24
check multinode_nbctl --gateway-port lr0-public-p2 lr-nat-add lr0 snat 20.0.2.2 10.0.2.0/24
check multinode_nbctl --gateway-port lr1-public-p3 lr-nat-add lr1 snat 20.0.2.3 40.0.2.0/24

M_NS_CHECK_EXEC([ovn-chassis-3], [sw1p1], [ping -q -c 3 -i 0.3 -w 2 20.0.2.1 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-3], [sw1p1], [ping -q -c 3 -i 0.3 -w 2 20.0.2.2 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Configure the second DGP for the lr1
# DGP public4
check multinode_nbctl lsp-add public ln-public-4
check multinode_nbctl lsp-set-type ln-public-4 localnet
check multinode_nbctl lsp-set-addresses ln-public-4 unknown
check multinode_nbctl lsp-set-options ln-public-4 network_name=public4

# Attach DGP public4 to GW-2 public4 (overlay connectivity)
m_as ovn-gw-4 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public4:br-ex

check multinode_nbctl lrp-add lr1 lr1-public-p4 40:54:00:00:00:04 20.0.2.4/24 2000::4/64
check multinode_nbctl lsp-add public public-lr1-p4
check multinode_nbctl lsp-set-type public-lr1-p4 router
check multinode_nbctl lsp-set-addresses public-lr1-p4 router
check multinode_nbctl lsp-set-options public-lr1-p4 router-port=lr1-public-p4
check multinode_nbctl lrp-set-gateway-chassis lr1-public-p4 ovn-gw-4 10

M_NS_CHECK_EXEC([ovn-chassis-3], [sw1p1], [ping -q -c 3 -i 0.3 -w 2 20.0.2.4 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

# Add SNAT rules using gateway-port
check multinode_nbctl --gateway-port lr1-public-p4 lr-nat-add lr1 snat 20.0.2.4 40.0.2.0/24

# Add a default route for multiple DGPs using ECMP - second step (multipath)
check multinode_nbctl --ecmp lr-route-add lr0 0.0.0.0/0 20.0.2.4
check multinode_nbctl --ecmp lr-route-add lr1 0.0.0.0/0 20.0.2.2

# Start backend http services
M_NS_DAEMONIZE([ovn-chassis-1], [sw0p1], [python3 -m http.server --bind 10.0.2.3 80 >/dev/null 2>&1], [http1.pid])
M_NS_DAEMONIZE([ovn-chassis-2], [sw0p2], [python3 -m http.server --bind 10.0.2.4 80 >/dev/null 2>&1], [http2.pid])

# wait for http server be ready
OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip netns exec sw0p1 ss -tulpn | grep LISTEN | grep 10.0.2.3:80])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip netns exec sw0p2 ss -tulpn | grep LISTEN | grep 10.0.2.4:80])

# create a big file on web servers for download
M_NS_EXEC([ovn-chassis-1], [sw0p1], [dd bs=512 count=200000 if=/dev/urandom of=download_file])
M_NS_EXEC([ovn-chassis-2], [sw0p2], [dd bs=512 count=200000 if=/dev/urandom of=download_file])

# create LB
check multinode_nbctl lb-add lb0 "172.16.0.100:80" "10.0.2.3:80,10.0.2.4:80"
check multinode_nbctl lr-lb-add lr0 lb0
check multinode_nbctl ls-lb-add sw0 lb0

check multinode_nbctl --wait=sb sync

# Flush conntrack entries for easier output parsing of next test.
m_as ovn-gw-1 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-2 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-3 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-4 ovs-appctl dpctl/flush-conntrack

# Check direct backend traffic using the same LB ports
M_NS_EXEC([ovn-chassis-3], [sw1p1], [sh -c 'curl -v -O 10.0.2.3:80/download_file --retry 0 --connect-timeout 1 --max-time 1 --local-port 59013 2>curl.out'])

gw1_ct=$(m_as ovn-gw-1 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
gw2_ct=$(m_as ovn-gw-2 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
gw3_ct=$(m_as ovn-gw-3 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
gw4_ct=$(m_as ovn-gw-4 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')

# Check the backend IP from ct entries on gateways
backend_check_gw1=$(echo -e $gw1_ct | grep "dport=80" | grep "59013" -c)
backend_check_gw2=$(echo -e $gw2_ct | grep "dport=80" | grep "59013" -c)
backend_check_gw3=$(echo -e $gw3_ct | grep "dport=80" | grep "59013" -c)
backend_check_gw4=$(echo -e $gw4_ct | grep "dport=80" | grep "59013" -c)

chassis_in_use=$(($backend_check_gw1 + $backend_check_gw2 + $backend_check_gw3 + $backend_check_gw4))

# If the traffic passes through both gateways (GW-1 and GW-2 OR GW-3 and GW-4) it will be dropped because
# we are bypassing the Stateless NAT solution for LB when we access the backend directly
if [[ $chassis_in_use -gt 2 ]]; then
OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 ip netns exec sw1p1 cat -v curl.out | \
sed 's/\(.*\)timed out/timed out\n/' | sed 's/\(.*\)connect timeout/timed out\n/' | grep -i -e "timed out" | uniq], [0], [dnl
timed out
])
else
OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 cat -v curl.out | M_FORMAT_CURL([10.0.2.3], [80])], [0], [dnl
Connected to 10.0.2.3 (10.0.2.3) port 80
200 OK
])
fi

# Flush conntrack entries for easier output parsing of next test.
m_as ovn-gw-1 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-2 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-3 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-4 ovs-appctl dpctl/flush-conntrack

M_NS_EXEC([ovn-chassis-3], [sw1p1], [sh -c 'curl -v -O 10.0.2.4:80/download_file --retry 0 --connect-timeout 1 --max-time 1 --local-port 59014 2>curl.out'])

gw1_ct=$(m_as ovn-gw-1 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
gw2_ct=$(m_as ovn-gw-2 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
gw3_ct=$(m_as ovn-gw-3 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
gw4_ct=$(m_as ovn-gw-4 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')

# Check the backend IP from ct entries on gateways
backend_check_gw1=$(echo -e $gw1_ct | grep "dport=80" | grep "59014" -c)
backend_check_gw2=$(echo -e $gw2_ct | grep "dport=80" | grep "59014" -c)
backend_check_gw3=$(echo -e $gw3_ct | grep "dport=80" | grep "59014" -c)
backend_check_gw4=$(echo -e $gw4_ct | grep "dport=80" | grep "59014" -c)

chassis_in_use=$(($backend_check_gw1 + $backend_check_gw2 + $backend_check_gw3 + $backend_check_gw4))

# If the traffic passes through both gateways (GW-1 and GW-2 OR GW-3 and GW-4) it will be dropped because
# we are bypassing the Stateless NAT solution for LB when we access the backend directly
if [[ $chassis_in_use -gt 2 ]]; then
OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 ip netns exec sw1p1 cat -v curl.out | \
sed 's/\(.*\)timed out/timed out\n/' | sed 's/\(.*\)connect timeout/timed out\n/' | grep -i -e "timed out" | uniq], [0], [dnl
timed out
])
else
OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 cat -v curl.out | M_FORMAT_CURL([10.0.2.4], [80])], [0], [dnl
Connected to 10.0.2.4 (10.0.2.4) port 80
200 OK
])
fi

# Check the flows again for the LB VIP
M_NS_EXEC([ovn-chassis-3], [sw1p1], [sh -c 'curl -v 172.16.0.100:80 --retry 0 --connect-timeout 1 --max-time 1 --local-port 59015 2>curl.out'])

curl_timeout=$(m_as ovn-chassis-3 cat -v curl.out | grep -i -e "timed out" -e "timeout" -c)

# This may fail because we do not have the flows to work independently of the DGP (DNAT + SNAT for the LB Stateless NAT)
if [[ $curl_timeout -gt 0 ]]; then
OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 ip netns exec sw1p1 cat -v curl.out | \
sed 's/\(.*\)timed out/timed out\n/' | sed 's/\(.*\)connect timeout/timed out\n/' | grep -i -e "timed out" | uniq], [0], [dnl
timed out
])
else
OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 cat -v curl.out | M_FORMAT_CURL([172.16.0.100], [80])], [0], [dnl
Connected to 172.16.0.100 (172.16.0.100) port 80
200 OK
])
fi

# Check the flows again for the LB VIP
M_NS_EXEC([ovn-chassis-3], [sw1p1], [sh -c 'curl -v 172.16.0.100:80 --retry 0 --connect-timeout 1 --max-time 1 --local-port 59016 2>curl.out'])

curl_timeout=$(m_as ovn-chassis-3 cat -v curl.out | grep -i -e "timed out" -e "timeout" -c)

# This may fail because we do not have the flows to work independently of the DGP (DNAT + SNAT for the LB Stateless NAT)
if [[ $curl_timeout -gt 0 ]]; then
OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 ip netns exec sw1p1 cat -v curl.out | \
sed 's/\(.*\)timed out/timed out\n/' | sed 's/\(.*\)connect timeout/timed out\n/' | grep -i -e "timed out" | uniq], [0], [dnl
timed out
])
else
OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 cat -v curl.out | M_FORMAT_CURL([172.16.0.100], [80])], [0], [dnl
Connected to 172.16.0.100 (172.16.0.100) port 80
200 OK
])
fi

# Set use_stateless_nat to true
# Now, if the traffic passes through both gateways (GW-1 and GW-2) it will be forwarded successfully
check multinode_nbctl set load_balancer lb0 options:use_stateless_nat=true

# Check the flows again for the LB VIP - always needs to be successful regardless of the datapath (one or two gw chassis)
M_NS_EXEC([ovn-chassis-3], [sw1p1], [sh -c 'curl -v -O 172.16.0.100:80/download_file --retry 0 --connect-timeout 1 --max-time 1 2>curl.out'])

OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 cat -v curl.out | M_FORMAT_CURL([172.16.0.100], [80])], [0], [dnl
Connected to 172.16.0.100 (172.16.0.100) port 80
200 OK
])

M_NS_EXEC([ovn-chassis-3], [sw1p1], [sh -c 'curl -v -O 172.16.0.100:80/download_file --retry 0 --connect-timeout 1 --max-time 1 2>curl.out'])

OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 cat -v curl.out | M_FORMAT_CURL([172.16.0.100], [80])], [0], [dnl
Connected to 172.16.0.100 (172.16.0.100) port 80
200 OK
])

M_NS_EXEC([ovn-chassis-3], [sw1p1], [sh -c 'curl -v -O 172.16.0.100:80/download_file --retry 0 --connect-timeout 1 --max-time 1 2>curl.out'])

OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 cat -v curl.out | M_FORMAT_CURL([172.16.0.100], [80])], [0], [dnl
Connected to 172.16.0.100 (172.16.0.100) port 80
200 OK
])

M_NS_EXEC([ovn-chassis-3], [sw1p1], [sh -c 'curl -v -O 172.16.0.100:80/download_file --retry 0 --connect-timeout 1 --max-time 1 2>curl.out'])

OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 cat -v curl.out | M_FORMAT_CURL([172.16.0.100], [80])], [0], [dnl
Connected to 172.16.0.100 (172.16.0.100) port 80
200 OK
])

# Direct backend traffic using the same LB ports needs to be dropped
M_NS_EXEC([ovn-chassis-3], [sw1p1], [sh -c 'curl -v -O 10.0.2.3:80/download_file --retry 0 --connect-timeout 1 --max-time 1 2>curl.out'])

OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 ip netns exec sw1p1 cat -v curl.out | \
sed 's/\(.*\)timed out/timed out\n/' | sed 's/\(.*\)connect timeout/timed out\n/' | grep -i -e "timed out" | uniq], [0], [dnl
timed out
])

# check again using another source ports
M_NS_EXEC([ovn-chassis-3], [sw1p1], [sh -c 'curl -v -O 10.0.2.3:80/download_file --retry 0 --connect-timeout 1 --max-time 1 2>curl.out'])

OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 ip netns exec sw1p1 cat -v curl.out | \
sed 's/\(.*\)timed out/timed out\n/' | sed 's/\(.*\)connect timeout/timed out\n/' | grep -i -e "timed out" | uniq], [0], [dnl
timed out
])

# Start backend http services using different ports from the LB config - check connectivity
M_NS_DAEMONIZE([ovn-chassis-1], [sw0p1], [python3 -m http.server --bind 10.0.2.3 8080 >/dev/null 2>&1], [http3.pid])
M_NS_DAEMONIZE([ovn-chassis-2], [sw0p2], [python3 -m http.server --bind 10.0.2.4 8080 >/dev/null 2>&1], [http4.pid])

# wait for http server be ready
OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip netns exec sw0p1 ss -tulpn | grep LISTEN | grep 10.0.2.3:8080])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip netns exec sw0p2 ss -tulpn | grep LISTEN | grep 10.0.2.4:8080])

# Flush conntrack entries for easier output parsing of next test.
m_as ovn-gw-1 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-2 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-3 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-4 ovs-appctl dpctl/flush-conntrack

M_NS_EXEC([ovn-chassis-3], [sw1p1], [sh -c 'curl -v -O 10.0.2.4:8080/download_file --retry 0 --connect-timeout 1 --max-time 1 --local-port 59017 2>curl.out'])

gw1_ct=$(m_as ovn-gw-1 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
gw2_ct=$(m_as ovn-gw-2 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
gw3_ct=$(m_as ovn-gw-3 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
gw4_ct=$(m_as ovn-gw-4 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')

# Check the backend IP from ct entries on gateways
backend_check_gw1=$(echo -e $gw1_ct | grep "dport=8080" | grep "59017" -c)
backend_check_gw2=$(echo -e $gw2_ct | grep "dport=8080" | grep "59017" -c)
backend_check_gw3=$(echo -e $gw3_ct | grep "dport=8080" | grep "59017" -c)
backend_check_gw4=$(echo -e $gw4_ct | grep "dport=8080" | grep "59017" -c)

chassis_in_use=$(($backend_check_gw1 + $backend_check_gw2 + $backend_check_gw3 + $backend_check_gw4))

# If the traffic passes through both gateways (GW-1 and GW-2 OR GW-3 and GW-4) it will be dropped because
# we are bypassing the Stateless NAT solution for LB when we access the backend directly
if [[ $chassis_in_use -gt 2 ]]; then
OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 ip netns exec sw1p1 cat -v curl.out | \
sed 's/\(.*\)timed out/timed out\n/' | sed 's/\(.*\)connect timeout/timed out\n/' | grep -i -e "timed out" | uniq], [0], [dnl
timed out
])
else
OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 cat -v curl.out | M_FORMAT_CURL([10.0.2.4], [8080])], [0], [dnl
Connected to 10.0.2.4 (10.0.2.4) port 8080
200 OK
])
fi

# Flush conntrack entries for easier output parsing of next test.
m_as ovn-gw-1 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-2 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-3 ovs-appctl dpctl/flush-conntrack
m_as ovn-gw-4 ovs-appctl dpctl/flush-conntrack

# Check again
M_NS_EXEC([ovn-chassis-3], [sw1p1], [sh -c 'curl -v -O 10.0.2.4:8080/download_file --retry 0 --connect-timeout 1 --max-time 1 --local-port 59018 2>curl.out'])

gw1_ct=$(m_as ovn-gw-1 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
gw2_ct=$(m_as ovn-gw-2 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
gw3_ct=$(m_as ovn-gw-3 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')
gw4_ct=$(m_as ovn-gw-4 ovs-appctl dpctl/dump-conntrack | sed ':a;N;$!ba;s/\n/\\n/g')

# Check the backend IP from ct entries on gateways
backend_check_gw1=$(echo -e $gw1_ct | grep "dport=8080" | grep "59018" -c)
backend_check_gw2=$(echo -e $gw2_ct | grep "dport=8080" | grep "59018" -c)
backend_check_gw3=$(echo -e $gw3_ct | grep "dport=8080" | grep "59018" -c)
backend_check_gw4=$(echo -e $gw4_ct | grep "dport=8080" | grep "59018" -c)

chassis_in_use=$(($backend_check_gw1 + $backend_check_gw2 + $backend_check_gw3 + $backend_check_gw4))

# If the traffic passes through both gateways (GW-1 and GW-2 OR GW-3 and GW-4) it will be dropped because
# we are bypassing the Stateless NAT solution for LB when we access the backend directly
if [[ $chassis_in_use -gt 2 ]]; then
OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 ip netns exec sw1p1 cat -v curl.out | \
sed 's/\(.*\)timed out/timed out\n/' | sed 's/\(.*\)connect timeout/timed out\n/' | grep -i -e "timed out" | uniq], [0], [dnl
timed out
])
else
OVS_WAIT_FOR_OUTPUT([m_as ovn-chassis-3 cat -v curl.out | M_FORMAT_CURL([10.0.2.4], [8080])], [0], [dnl
Connected to 10.0.2.4 (10.0.2.4) port 8080
200 OK
])
fi

AT_CLEANUP

AT_SETUP([ovn multinode - Transit Router basic functionality])

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

# Network topology
#    ┌─────────────────────────────────┐     ┌────────────────────────────────┐
#    │                                 │     │                                │
#    │    ┌───────────────────┐   AZ1  │     │  AZ2   ┌───────────────────┐   │
#    │    │     external      │        │     │        │                   │   │
#    │    │                   │        │     │        │                   │   │
#    │    │ 192.168.100.10/24 │        │     │        │ ................. │   │
#    │    │    1000::10/64    │        │     │        │                   │   │
#    │    └─────────┬─────────┘        │     │        └─────────┬─────────┘   │
#    │              │                  │     │                  │             │
#    │              │                  │     │                  │             │
#    │    ┌─────────┴─────────┐        │     │        ┌─────────┴─────────┐   │
#    │    │ 192.168.100.1/24  │        │     │        │ 192.168.100.2/24  │   │
#    │    │    1000::1/64     │        │     │        │    1000::2/64     │   │
#    │    │                   │        │     │        │                   │   │
#    │    │        GW         │        │     │        │        GW         │   │
#    │    │                   │        │     │        │                   │   │
#    │    │   100.65.0.1/30   │        │     │        │   100.65.0.5/30   │   │
#    │    │   100:65::1/126   │        │     │        │   100:65::5/126   │   │
#    │    └─────────┬─────────┘        │     │        └───────────────────┘   │
#    │              │                  │     │                  │             │
#    │              │  Peer ports      │     │                  │  Peer ports │
#    │              │                  │     │                  │             │
#    │    ┌─────────┴──────────────────│─────│──────────────────┴─────────┐   │
#    │    │   100.65.0.2/30            │     │            100.65.0.6/30   │   │
#    │    │   100:65::2/126            │     │            100:65::6/126   │   │
#    │    │                            │     │                            │   │
#    │    │                            │  TR │                            │   │
#    │    │                            │     │                            │   │
#    │    │  10.100.200.1/24           │     │           10.100.200.1/24  │   │
#    │    │   10:200::1/64             │     │            10:200::1/64    │   │
#    │    └─────────┬──────────────────│─────│────────────────────────────┘   │
#    │              │                  │     │                  │             │
#    │              │                  │     │                  │             │
#    │              │                  │     │                  │             │
#    │    ┌─────────┴──────────────────│─────│────────────────────────────┐   │
#    │    │                            │  TS │                            │   │
#    │    └─────────┬──────────────────│─────│────────────────────────────┘   │
#    │              │                  │     │                  │             │
#    │              │                  │     │                  │             │
#    │              │                  │     │                  │             │
#    │    ┌─────────┴─────────┐        │     │        ┌─────────┴─────────┐   │
#    │    │       pod10       │        │     │        │       pod20       │   │
#    │    │                   │        │     │        │                   │   │
#    │    │  10.100.200.10/24 │        │     │        │  10.100.200.20/24 │   │
#    │    │   10:200::10/64   │        │     │        │   10:200::20/64   │   │
#    │    └───────────────────┘        │     │        └───────────────────┘   │
#    └─────────────────────────────────┘     └────────────────────────────────┘

for i in 1 2; do
    chassis="ovn-chassis-$i"
    ip=$(m_as $chassis ip -4 addr show eth1 | grep inet | awk '{print $2}' | cut -d'/' -f1)

    multinode_setup_northd $chassis
    multinode_setup_controller $chassis $chassis $ip $ip

    check m_as $chassis ovs-vsctl set open . external_ids:ovn-monitor-all=true
    check m_as $chassis ovs-vsctl set open . external_ids:ovn-is-interconn=true

    check m_as $chassis ovn-nbctl ls-add public

    check m_as $chassis ovn-nbctl lsp-add public public-gw
    check m_as $chassis ovn-nbctl lsp-set-type public-gw router
    check m_as $chassis ovn-nbctl lsp-set-addresses public-gw router
    check m_as $chassis ovn-nbctl lsp-set-options public-gw router-port=gw-public

    check m_as $chassis ovn-nbctl lr-add gw
    check m_as $chassis ovn-nbctl lrp-add gw gw-public 00:00:00:00:20:00 192.168.100.$i/24 1000::$i/64

    check m_as $chassis ovn-nbctl set logical_router gw options:chassis=$chassis

    # Add TR and set the same tunnel key for both chassis
    check m_as $chassis ovn-nbctl ls-add ts
    check m_as $chassis ovn-nbctl set logical_switch ts other_config:requested-tnl-key=10

    check m_as $chassis ovn-nbctl lsp-add ts ts-tr
    check m_as $chassis ovn-nbctl lsp-set-type ts-tr router
    check m_as $chassis ovn-nbctl lsp-set-addresses ts-tr router
    check m_as $chassis ovn-nbctl lsp-set-options ts-tr router-port=tr-ts

    check m_as $chassis ovn-nbctl lr-add tr
    check m_as $chassis ovn-nbctl lrp-add tr tr-ts 00:00:00:00:10:00 10.100.200.1/24 10:200::1/64
    check m_as $chassis ovn-nbctl set logical_router tr options:requested-tnl-key=20
    check m_as $chassis ovn-nbctl lrp-set-gateway-chassis tr-ts $chassis

    # Add TS pods, with the same tunnel keys on both sides
    check m_as $chassis ovn-nbctl lsp-add ts pod10
    check m_as $chassis ovn-nbctl lsp-set-addresses pod10 "00:00:00:00:10:10 10.100.200.10 10:200::10"
    check m_as $chassis ovn-nbctl set logical_switch_port pod10 options:requested-tnl-key=10

    check m_as $chassis ovn-nbctl lsp-add ts pod20
    check m_as $chassis ovn-nbctl lsp-set-addresses pod20 "00:00:00:00:10:20 10.100.200.20 10:200::20"
    check m_as $chassis ovn-nbctl set logical_switch_port pod20 options:requested-tnl-key=20

    # Add mgmt pod
    check m_as $chassis ovn-nbctl lsp-add ts mgmt
    check m_as $chassis ovn-nbctl lsp-set-addresses mgmt "00:00:00:00:10:11 10.100.200.11 10:200::11"
    check m_as $chassis ovn-nbctl set logical_switch_port mgmt options:requested-tnl-key=11
done

# Add SNAT for the GW router that corresponds to "gw-tr" LRP IP
check m_as ovn-chassis-1 ovn-nbctl lr-nat-add gw snat 100.65.0.1 192.168.100.0/24
check m_as ovn-chassis-1 ovn-nbctl lr-nat-add gw snat 100:65::1 1000::/64
check m_as ovn-chassis-2 ovn-nbctl lr-nat-add gw snat 100.65.0.5 192.168.100.0/24
check m_as ovn-chassis-2 ovn-nbctl lr-nat-add gw snat 100:65::5 1000::/64

# Add peer ports between GW and TR
check m_as ovn-chassis-1 ovn-nbctl lrp-add gw gw-tr 00:00:00:00:30:01 100.65.0.1/30 100:65::1/126 peer=tr-gw
check m_as ovn-chassis-1 ovn-nbctl lrp-add tr tr-gw 00:00:00:00:30:02 100.65.0.2/30 100:65::2/126 peer=gw-tr

check m_as ovn-chassis-2 ovn-nbctl lrp-add gw gw-tr 00:00:00:00:30:05 100.65.0.5/30 100:65::5/126 peer=tr-gw
check m_as ovn-chassis-2 ovn-nbctl lrp-add tr tr-gw 00:00:00:00:30:06 100.65.0.6/30 100:65::6/126 peer=gw-tr

# Add routes for the TS subnet
check m_as ovn-chassis-1 ovn-nbctl lr-route-add gw 10.100.200.0/24 100.65.0.2
check m_as ovn-chassis-1 ovn-nbctl lr-route-add gw 10:200::/64 100:65::2
check m_as ovn-chassis-2 ovn-nbctl lr-route-add gw 10.100.200.0/24 100.65.0.6
check m_as ovn-chassis-2 ovn-nbctl lr-route-add gw 10:200::/64 100:65::6

# Add LB on TS and condition NAT
check m_as ovn-chassis-1 ovn-nbctl lb-add lb0 172.16.0.5:5656 10.100.200.10:2324 tcp
check m_as ovn-chassis-1 ovn-nbctl ls-lb-add ts lb0
check m_as ovn-chassis-1 ovn-nbctl --match="eth.dst == 00:00:00:00:10:11" lr-nat-add tr snat 172.16.0.2 10.100.200.0/24
check m_as ovn-chassis-1 ovn-nbctl set logical_router tr options:ct-commit-all="true"

# Add mutual remote ports
check m_as ovn-chassis-1 ovn-nbctl lrp-add tr tr-az2 00:00:00:00:30:06 100.65.0.6/30 100:65::6/126
check m_as ovn-chassis-1 ovn-nbctl set logical_router_port tr-az2 options:requested-chassis=ovn-chassis-2

check m_as ovn-chassis-2 ovn-nbctl lrp-add tr tr-az1 00:00:00:00:30:02 100.65.0.2/30 100:65::2/126
check m_as ovn-chassis-2 ovn-nbctl set logical_router_port tr-az1 options:requested-chassis=ovn-chassis-1

# Important set the proper tunnel keys
check m_as ovn-chassis-1 ovn-nbctl set logical_router_port tr-gw options:requested-tnl-key=10
check m_as ovn-chassis-1 ovn-nbctl set logical_router_port tr-az2 options:requested-tnl-key=20

check m_as ovn-chassis-2 ovn-nbctl set logical_router_port tr-gw options:requested-tnl-key=20
check m_as ovn-chassis-2 ovn-nbctl set logical_router_port tr-az1 options:requested-tnl-key=10

check m_as ovn-chassis-1 ovn-nbctl lsp-add public external
check m_as ovn-chassis-1 ovn-nbctl lsp-set-addresses external "00:00:00:00:20:10 192.168.100.10 1000::10"

# Add mutual chassis
check m_as ovn-chassis-1 ovn-sbctl chassis-add ovn-chassis-2 geneve $(m_as ovn-chassis-2 ip -4 addr show eth1 | grep inet | awk '{print $2}' | cut -d'/' -f1)
check m_as ovn-chassis-1 ovn-sbctl set chassis ovn-chassis-2 other_config:is-remote=true

check m_as ovn-chassis-2 ovn-sbctl chassis-add ovn-chassis-1 geneve $(m_as ovn-chassis-1 ip -4 addr show eth1 | grep inet | awk '{print $2}' | cut -d'/' -f1)
check m_as ovn-chassis-2 ovn-sbctl set chassis ovn-chassis-1 other_config:is-remote=true

# Configure ports on the transit switch as remotes
check m_as ovn-chassis-1 ovn-nbctl lsp-set-type pod20 remote
check m_as ovn-chassis-1 ovn-nbctl lsp-set-options pod10 requested-chassis=ovn-chassis-1
check m_as ovn-chassis-1 ovn-nbctl lsp-set-options mgmt requested-chassis=ovn-chassis-1
check m_as ovn-chassis-1 ovn-nbctl lsp-set-options pod20 requested-chassis=ovn-chassis-2

check m_as ovn-chassis-2 ovn-nbctl lsp-set-type pod10 remote
check m_as ovn-chassis-2 ovn-nbctl lsp-set-type mgmt remote
check m_as ovn-chassis-2 ovn-nbctl lsp-set-options pod10 requested-chassis=ovn-chassis-1
check m_as ovn-chassis-2 ovn-nbctl lsp-set-options mgmt requested-chassis=ovn-chassis-1
check m_as ovn-chassis-2 ovn-nbctl lsp-set-options pod20 requested-chassis=ovn-chassis-2

m_as ovn-chassis-1 /data/create_fake_vm.sh external external 00:00:00:00:20:10 1500 192.168.100.10 24 192.168.100.1 1000::10/64 1000::1
m_as ovn-chassis-1 /data/create_fake_vm.sh pod10 pod10 00:00:00:00:10:10 1500 10.100.200.10 24 10.100.200.1 10:200::10/64 10:200::1
m_as ovn-chassis-1 /data/create_fake_vm.sh mgmt mgmt 00:00:00:00:10:11 1500 10.100.200.11 24 10.100.200.1 10:200::11/64 10:200::1
m_as ovn-chassis-2 /data/create_fake_vm.sh pod20 pod20 00:00:00:00:10:20 1500 10.100.200.20 24 10.100.200.1 10:200::20/64 10:200::1

# We cannot use any of the helpers as they assume that there is only single ovn-northd instance running
check m_as ovn-chassis-1 ovn-nbctl --wait=hv sync
OVS_WAIT_UNTIL([test -n "$(m_as ovn-chassis-1 ovn-sbctl --bare --columns _uuid find Port_Binding logical_port=external up=true)"])
OVS_WAIT_UNTIL([test -n "$(m_as ovn-chassis-1 ovn-sbctl --bare --columns _uuid find Port_Binding logical_port=pod10 up=true)"])
check m_as ovn-chassis-2 ovn-nbctl --wait=hv sync
OVS_WAIT_UNTIL([test -n "$(m_as ovn-chassis-2 ovn-sbctl --bare --columns _uuid find Port_Binding logical_port=pod20 up=true)"])

M_NS_CHECK_EXEC([ovn-chassis-1], [external], [ping -q -c 5 -i 0.3 -w 2 10.100.200.20 | FORMAT_PING], \
[0], [dnl
5 packets transmitted, 5 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-1], [external], [ping -q -c 5 -i 0.3 -w 2 10:200::20 | FORMAT_PING], \
[0], [dnl
5 packets transmitted, 5 received, 0% packet loss, time 0ms
])

M_NS_CHECK_EXEC([ovn-chassis-1], [mgmt], [ip a a 172.16.100.2/24 dev mgmt])
M_NS_DAEMONIZE([ovn-chassis-1], [pod10], [nc -e /bin/cat -v -l -o server.log 10.100.200.10 2324], [pod10.pid])
M_START_TCPDUMP([ovn-chassis-1], [-neei pod10-p ip], [pod10])
M_START_TCPDUMP([ovn-chassis-1], [-neei mgmt-p ip], [mgmt])

m_as ovn-chassis-1 sh -c 'echo -e "Hello\nHello" > msg.expected'
check m_as ovn-chassis-1 ovn-nbctl --policy="src-ip" lr-route-add tr 10.100.200.0/24 10.100.200.11

check test $(m_as ovn-chassis-1 grep -c "skipping output to input port" \
    /var/log/openvswitch/ovs-vswitchd.log) -eq 0
check test $(m_as ovn-chassis-2 grep -c "skipping output to input port" \
    /var/log/openvswitch/ovs-vswitchd.log) -eq 0

M_NS_CHECK_EXEC([ovn-chassis-1], [mgmt], [sh -c '(echo "Hello"; sleep 3) | nc -s 172.16.100.2 -o client.log 172.16.0.5 5656'], [0], [ignore], [ignore])
check m_as ovn-chassis-1 cmp server.log msg.expected
check m_as ovn-chassis-1 cmp client.log msg.expected

echo "Chassis1"
m_as ovn-chassis-1 ovn-sbctl show
m_as ovn-chassis-1 ovn-nbctl show
m_as ovn-chassis-1 ovs-vsctl show

echo "Chassis2"
m_as ovn-chassis-2 ovn-sbctl show
m_as ovn-chassis-2 ovn-nbctl show
m_as ovn-chassis-2 ovs-vsctl show

# Connect the chassis back to the original northd and remove northd per chassis.
for i in 1 2; do
    chassis="ovn-chassis-$i"
    ip=$(m_as $chassis ip -4 addr show eth1 | grep inet | awk '{print $2}' | cut -d'/' -f1)

    multinode_setup_controller $chassis $chassis $ip "170.168.0.2"
    multinode_cleanup_northd $chassis
done

m_as ovn-chassis-1 killall tcpdump

AT_CLEANUP

AT_SETUP([Migration of container ports])
# Migrate vif port between chassis-1 and chassis-3; send packets between
# chassis-2 and chassis-1/chassis-3, and check that
# - packet handing on src works before migration.
# - packet handing on src & dst works during migration.
# - packet handing on dst works after migration.
# Do the same for container ports.
# The container port migration is tested in two different orders,
# setting iface-id on dst resp. before and after requested-chassis.

# Check that ovn-fake-multinode setup is up and running
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

m_as ovn-chassis-1 ip link del migrator-p
m_as ovn-chassis-2 ip link del sw0-port1-p
m_as ovn-chassis-2 ip link del sw0-port2-p
m_as ovn-chassis-2 ip link del sw0-port3-p
m_as ovn-chassis-3 ip link del migrator-p

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-3 ip link show | grep -q genev_sys])

# check_ping(src_port, src_chassis, dst_port, expected_status)
# Check whether ping, from src_port on src_chassis to dst_port works as expected.
# expected_status:
#   - "success" (or empty): ping should succeed between src and dst.
#   - "potential-duplicates": ping should succeed, but we might have potential duplicate packets.
#   - "lost": ping should fail, no packets should go through.
# Source ip namespace is derived from previous parameters:
#   - parent_port name for container ports.
#   - same name as port itself for other ports.
# Dest ip is retrieved from dst_port.
check_ping() {
    src_port=$1
    src_chassis=$2
    dst_port=$3
    status=${4:-success}

    parent_port=$(multinode_sbctl get port_binding $src_port parent_port)
    if [[ "$parent_port" != "[]" ]]; then
        src_ns=$parent_port
    else
        src_ns=$src_port
    fi
    dst_chassis_uuid=$(multinode_sbctl get port_binding $dst_port chassis)
    requested_chassis_uuid=$(multinode_sbctl get port_binding $dst_port requested_chassis)
    dst_chassis=$(multinode_sbctl --bare --columns name list chassis $dst_chassis_uuid)
    dst_ip=$(multinode_nbctl lsp-get-addresses $dst_port | awk '{print $2}')
    echo "$src_port on $src_chassis => $dst_port on $dst_chassis(requested_chassis=$requested_chassis_uuid)"
    M_NS_CHECK_EXEC([$src_chassis], [$src_ns], [ping -q -c 3 -i 0.1 -w 2 $dst_ip | FORMAT_PING], \
[0], [stdout])
    if [[ "$status" == "success" ]]; then
        AT_CHECK([cat stdout | grep -c "3 packets transmitted, 3 received, 0% packet loss"], [0], [dnl
1
])
    elif [[ "$status" == "potential-duplicates" ]]; then
        AT_CHECK([cat stdout | grep "3 packets transmitted" | grep -c "3 received"], [0], [dnl
1
])
    elif [[ "$status" == "lost" ]]; then
        AT_CHECK([cat stdout | grep -c "100% packet loss"], [0], [dnl
1
])
    else
        echo "unexpected status $status"
        AT_FAIL_IF([:])
    fi
}

check multinode_nbctl ls-add sw0
check multinode_nbctl lsp-add sw0 migrator
check multinode_nbctl lsp-set-addresses migrator "50:54:00:00:00:09 10.0.0.9 1000::9"
for i in 1 2 3; do
    check multinode_nbctl lsp-add sw0 sw0-port${i}
    check multinode_nbctl lsp-set-addresses sw0-port${i} "50:54:00:00:00:0${i} 10.0.0.${i} 1000::${i}"
done

# Set requested chassis before creating migrator on chassis-3
check multinode_nbctl --wait=hv set Logical_Switch_Port migrator options:requested-chassis=ovn-chassis-1

m_as ovn-chassis-1 /data/create_fake_vm.sh migrator migrator 50:54:00:00:00:09 1342 10.0.0.9 24 10.0.0.10 1000::9/64 1000::a
m_as ovn-chassis-3 /data/create_fake_vm.sh migrator migrator 50:54:00:00:00:09 1342 10.0.0.9 24 10.0.0.10 1000::9/64 1000::a
for i in 1 2 3; do
    m_as ovn-chassis-${i} /data/create_fake_vm.sh sw0-port${i} sw0-port${i} 50:54:00:00:00:0${i} 1342 10.0.0.${i} 24 10.0.0.10 1000::${i}/64 1000::a
done

m_wait_for_ports_up

M_START_TCPDUMP([ovn-chassis-1], [-neei genev_sys_6081 arp or ip], [ch1_genev])
M_START_TCPDUMP([ovn-chassis-1], [-neei migrator-p arp or ip], [ch1_migrator])
M_START_TCPDUMP([ovn-chassis-2], [-neei genev_sys_6081 arp or ip], [ch2_genev])
for i in 1 2 3; do
    M_START_TCPDUMP([ovn-chassis-${i}], [-neei sw0-port${i}-p arp or ip], [ch${i}_sw0-port${i}])
done
M_START_TCPDUMP([ovn-chassis-3], [-neei genev_sys_6081 arp or ip], [ch3_genev])
M_START_TCPDUMP([ovn-chassis-3], [-neei migrator-p arp or ip], [ch3_migrator])

AS_BOX([Migration with vifs])
for i in 1 2 3; do
    check_ping migrator ovn-chassis-1 sw0-port${i}
    check_ping sw0-port${i} ovn-chassis-${i} migrator
done

echo "== Starting migration =="
check multinode_nbctl --wait=hv set Logical_Switch_Port migrator options:requested-chassis=ovn-chassis-1,ovn-chassis-3

for i in 1 2 3; do
    check_ping migrator ovn-chassis-1 sw0-port${i}
    check_ping migrator ovn-chassis-3 sw0-port${i}
    check_ping sw0-port${i} ovn-chassis-${i} migrator "potential-duplicates"
done

echo "== Finalizing migration =="
check multinode_nbctl --wait=hv set Logical_Switch_Port migrator options:requested-chassis=ovn-chassis-3

for i in 1 2 3; do
    check_ping migrator ovn-chassis-1 sw0-port${i} "lost"
    check_ping migrator ovn-chassis-3 sw0-port${i}
    check_ping sw0-port${i} ovn-chassis-${i} migrator
done

AS_BOX([Migration with container ports])
# Create container ports.
check multinode_nbctl ls-add sw1
check multinode_nbctl lsp-add sw1 mig-cont migrator 10 \
                   -- lsp-set-addresses mig-cont "f0:00:00:01:02:09 20.0.0.9"
for i in 1 2 3; do
    check multinode_nbctl lsp-add sw1 cont${i} sw0-port${i} 10 \
                       -- lsp-set-addresses cont${i} "f0:00:00:01:02:0${i} 20.0.0.${i}"
done

# Create the interface for lport mig-cont
M_NS_CHECK_EXEC([ovn-chassis-1], [migrator], [ip link add link migrator name cont type vlan id 10], [0])
M_NS_CHECK_EXEC([ovn-chassis-1], [migrator], [ip link set cont address f0:00:00:01:02:09], [0])
M_NS_CHECK_EXEC([ovn-chassis-1], [migrator], [ip link set cont up], [0])
M_NS_CHECK_EXEC([ovn-chassis-1], [migrator], [ip addr add 20.0.0.9/24 dev cont], [0])

M_NS_CHECK_EXEC([ovn-chassis-3], [migrator], [ip link add link migrator name cont type vlan id 10], [0])
M_NS_CHECK_EXEC([ovn-chassis-3], [migrator], [ip link set cont address f0:00:00:01:02:09], [0])
M_NS_CHECK_EXEC([ovn-chassis-3], [migrator], [ip link set cont up], [0])
M_NS_CHECK_EXEC([ovn-chassis-3], [migrator], [ip addr add 20.0.0.9/24 dev cont], [0])

# Create the cont interface for lport sw0-port1, sw0-port2, sw0-port3
for i in 1 2 3; do
    M_NS_CHECK_EXEC([ovn-chassis-${i}], [sw0-port${i}], [ip link add link sw0-port${i} name cont${i} type vlan id 10], [0])
    M_NS_CHECK_EXEC([ovn-chassis-${i}], [sw0-port${i}], [ip link set cont${i} address f0:00:00:01:02:0${i}], [0])
    M_NS_CHECK_EXEC([ovn-chassis-${i}], [sw0-port${i}], [ip link set cont${i} up], [0])
    M_NS_CHECK_EXEC([ovn-chassis-${i}], [sw0-port${i}], [ip addr add 20.0.0.${i}/24 dev cont${i}], [0])
done

for i in 1 2 3; do
    check_ping mig-cont ovn-chassis-3 cont${i}
    check_ping cont${i} ovn-chassis-${i} mig-cont
    check_ping migrator ovn-chassis-3 cont${i}
    check_ping cont${i} ovn-chassis-${i} migrator
done

echo "== Starting migration back =="
check multinode_nbctl --wait=hv set Logical_Switch_Port migrator options:requested-chassis=ovn-chassis-3,ovn-chassis-1

for i in 1 2 3; do
    check_ping mig-cont ovn-chassis-3 cont${i}
    check_ping mig-cont ovn-chassis-1 cont${i}
    check_ping cont${i} ovn-chassis-${i} mig-cont "potential-duplicates"
    check_ping migrator ovn-chassis-3 cont${i}
    check_ping migrator ovn-chassis-1 cont${i}
    check_ping cont${i} ovn-chassis-${i} migrator "potential-duplicates"
done

echo "== Finalizing migration =="
check multinode_nbctl --wait=hv set Logical_Switch_Port migrator options:requested-chassis=ovn-chassis-1

for i in 1 2 3; do
    check_ping mig-cont ovn-chassis-3 cont${i} "lost"
    check_ping mig-cont ovn-chassis-1 cont${i}
    check_ping cont${i} ovn-chassis-${i} mig-cont
    check_ping migrator ovn-chassis-3 cont${i} "lost"
    check_ping migrator ovn-chassis-1 cont${i}
    check_ping cont${i} ovn-chassis-${i} migrator
done

echo "== Starting another migration, this time before starting dst VM =="
# Unbind migrator from chassis-3
m_as ovn-chassis-3 ovs-vsctl -- set Interface migrator-p external_ids:iface-id=foo

check multinode_nbctl --wait=hv set Logical_Switch_Port migrator options:requested-chassis=ovn-chassis-1,ovn-chassis-3
sleep 1
m_as ovn-chassis-3 ovs-vsctl -- set Interface migrator-p external_ids:iface-id=migrator

for i in 1 2 3; do
    check_ping mig-cont ovn-chassis-3 cont${i}
    check_ping mig-cont ovn-chassis-1 cont${i}
    check_ping cont${i} ovn-chassis-${i} mig-cont "potential-duplicates"
    check_ping migrator ovn-chassis-3 cont${i}
    check_ping migrator ovn-chassis-1 cont${i}
    check_ping cont${i} ovn-chassis-${i} migrator "potential-duplicates"
done

echo "== Finalizing migration =="
check multinode_nbctl --wait=hv set Logical_Switch_Port migrator options:requested-chassis=ovn-chassis-3

for i in 1 2 3; do
    check_ping mig-cont ovn-chassis-1 cont${i} "lost"
    check_ping mig-cont ovn-chassis-3 cont${i}
    check_ping cont${i} ovn-chassis-${i} mig-cont
    check_ping migrator ovn-chassis-1 cont${i} "lost"
    check_ping migrator ovn-chassis-3 cont${i}
    check_ping cont${i} ovn-chassis-${i} migrator
done

# Remove iface-id from src after migration is completed
m_as ovn-chassis-1 ovs-vsctl -- remove Interface migrator-p external_ids iface-id
check multinode_nbctl --wait=sb sync

for i in 1 2 3; do
    check_ping migrator ovn-chassis-1 sw0-port${i} "lost"
    check_ping mig-cont ovn-chassis-1 cont${i} "lost"
    check_ping migrator ovn-chassis-1 cont${i} "lost"
    check_ping migrator ovn-chassis-3 sw0-port${i}
    check_ping mig-cont ovn-chassis-3 cont${i}
    check_ping migrator ovn-chassis-3 cont${i}
    check_ping sw0-port${i} ovn-chassis-${i} migrator
    check_ping cont${i} ovn-chassis-${i} mig-cont
    check_ping cont${i} ovn-chassis-${i} migrator
done

# Finally, remove interface from src after migration is completed
m_as ovn-chassis-1 ovs-vsctl -- del-port migrator-p
for i in 1 2 3; do
    check_ping migrator ovn-chassis-1 sw0-port${i} "lost"
    check_ping mig-cont ovn-chassis-1 cont${i} "lost"
    check_ping migrator ovn-chassis-1 cont${i} "lost"
    check_ping migrator ovn-chassis-3 sw0-port${i}
    check_ping mig-cont ovn-chassis-3 cont${i}
    check_ping migrator ovn-chassis-3 cont${i}
    check_ping sw0-port${i} ovn-chassis-${i} migrator
    check_ping cont${i} ovn-chassis-${i} mig-cont
    check_ping cont${i} ovn-chassis-${i} migrator
done

# Also do some recomputes ...
AS_BOX([Recompute after removing interface])
for i in 1 2 3; do
    m_as ovn-chassis-${i} ovn-appctl -t ovn-controller recompute
done

check multinode_nbctl --wait=sb sync

for i in 1 2 3; do
    check_ping migrator ovn-chassis-1 sw0-port${i} "lost"
    check_ping mig-cont ovn-chassis-1 cont${i} "lost"
    check_ping migrator ovn-chassis-1 cont${i} "lost"
    check_ping migrator ovn-chassis-3 sw0-port${i}
    check_ping mig-cont ovn-chassis-3 cont${i}
    check_ping migrator ovn-chassis-3 cont${i}
    check_ping sw0-port${i} ovn-chassis-${i} migrator
    check_ping cont${i} ovn-chassis-${i} mig-cont
    check_ping cont${i} ovn-chassis-${i} migrator
done

m_as ovn-chassis-1 killall tcpdump
m_as ovn-chassis-2 killall tcpdump
m_as ovn-chassis-3 killall tcpdump

AT_CLEANUP

AT_SETUP([ovn multinode bgp unnumbered])

# Check that ovn-fake-multinode setup is up and running.
check_fake_multinode_setup

CHECK_VRF()

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

CHECK_VRF()

add_guest_vm_and_connections() {
    node=$1
    vrf_id=$2
    default_route_mac=$3
    default_route=$4
    default_route_gw=$5
    guest_gw_ip=$6
    guest_ip=$7
    gw_router=$(m_ovn_frr_router_name $node)
    gw_router_lrp=$(m_ovn_frr_router_port_name $node)

    gw_lr=$(m_ovn_frr_router_name $node)
    lrp_to_join=lrp-$node-to-join
    lsp_join_to_lrp=join-to-lrp-$node
    lrp_guest=lrp-guest-$node

    ls_g=ls-guest-$node
    lsp_g_lrg=lsp-guest-$node-lr-guest
    lsp_g_iface=lsp-guest-$node-guest-vm
    lrp_g_lsg=lrp-guest-ls-guest-$node

    guest_gw_cidr="$guest_gw_ip/24"
    guest_cidr="$guest_ip/24"

    # set up connections to connect the new vm
    check multinode_nbctl lrp-add $gw_lr $lrp_to_join $default_route_mac
    check multinode_nbctl lrp-set-options $lrp_to_join \
        dynamic-routing-redistribute=nat
    check multinode_nbctl lsp-add $join_ls $lsp_join_to_lrp
    check multinode_nbctl lsp-set-type $lsp_join_to_lrp router
    check multinode_nbctl lsp-set-options $lsp_join_to_lrp \
        router-port=$lrp_to_join
    check multinode_nbctl lsp-set-addresses $lsp_join_to_lrp router

    check multinode_nbctl ls-add $ls_g
    check multinode_nbctl lrp-add $lr_guest $lrp_g_lsg \
        00:16:03:01:03:03 $guest_gw_cidr
    check multinode_nbctl lsp-add $ls_g $lsp_g_lrg
    check multinode_nbctl lsp-set-type $lsp_g_lrg router
    check multinode_nbctl lsp-set-options $lsp_g_lrg router-port=$lrp_g_lsg
    check multinode_nbctl lsp-set-addresses $lsp_g_lrg router
    check multinode_nbctl lsp-add $ls_g $lsp_g_iface
    check multinode_nbctl lsp-set-addresses $lsp_g_iface \
        '00:16:01:00:02:02 '$guest_cidr''

    # create the new vm
    m_as $node /data/create_fake_vm.sh $lsp_g_iface $guest_vm_ns \
        00:16:01:00:02:02 1342 $guest_ip 24 $guest_gw_ip 1000::13/64 1000::a
    neighbor_lla=$(m_as $node vtysh -c "show bgp vrf ovnvrf${vrf_id} neighbor ext0-bgp" | grep "^Foreign host:" | awk '{print $3}' | tr -d ',')

    check multinode_nbctl lr-route-add $gw_router "0.0.0.0/0" \
        $neighbor_lla $gw_router_lrp
    check multinode_nbctl lr-route-add $lr_guest \
        $default_route $default_route_gw $lrp_guest_join
}

m_setup_external_frr_router ovn-gw-1 4200000100 41.41.41.41 41.41.41.41/32
m_setup_ovn_frr_router      ovn-gw-1 4210000000 14.14.14.14 12:fb:d6:66:99:1c 10

m_setup_external_frr_router ovn-gw-2 4200000200 42.42.42.42 42.42.42.42/32
m_setup_ovn_frr_router      ovn-gw-2 4210000000 24.24.24.24 22:fb:d6:66:99:2c 20

OVS_WAIT_UNTIL([m_as ovn-gw-2 vtysh -c 'show bgp vrf ovnvrf20 neighbors' | grep -qE 'Connections established 1'])
OVS_WAIT_UNTIL([m_as ovn-gw-1 vtysh -c 'show bgp vrf ovnvrf10 neighbors' | grep -qE 'Connections established 1'])

# Tor <-> ovn-gw via bgp
# lr-guest with distributed gateway port
# bgp on lr-ovn-gw-2-ext0
#
#                guest-1          guest-2
#                       \        /
#                        lr-guest
#                          DGP
#                           |
#                        ls-join
#                       /       \
# tor <-> lr-ovn-gw-2-ext0*    lr-ovn-gw-1-ext0* <-> tor
#               |                     |
#         ls-ovn-gw-2-ext0     ls-ovn-gw-1-ext0
#
#
#

join_ls="ls-join"
lsp_join_guest="lsp-join-guest"

lr_guest="lr-guest"
lrp_guest_join="lrp-guest-join-dgp"

guest_vm_iface="guest-vm"
guest_vm_ns="ns-guest"

check multinode_nbctl ls-add $join_ls

check multinode_nbctl lr-add $lr_guest
check multinode_nbctl lrp-add $lr_guest $lrp_guest_join 00:16:06:12:f0:0d
check multinode_nbctl lsp-add $join_ls $lsp_join_guest
check multinode_nbctl lsp-set-type $lsp_join_guest router
check multinode_nbctl lsp-set-options $lsp_join_guest \
    router-port=$lrp_guest_join
check multinode_nbctl lsp-set-addresses $lsp_join_guest router
check multinode_nbctl lrp-set-gateway-chassis $lrp_guest_join ovn-gw-1 20
check multinode_nbctl lrp-set-gateway-chassis $lrp_guest_join ovn-gw-2 20

vrf_id=10
default_route_mac=00:00:ff:00:00:01
default_route=41.0.0.0/8
default_route_gw=fe80::200:ffff:fe00:1
guest_gw_ip=192.168.10.1
guest_ip=192.168.10.10
add_guest_vm_and_connections ovn-gw-1 $vrf_id           \
    $default_route_mac $default_route $default_route_gw \
    $guest_gw_ip $guest_ip

vrf_id=20
default_route_mac=00:00:ff:00:00:02
default_route=42.0.0.0/8
default_route_gw=fe80::200:ffff:fe00:2
guest_gw_ip=192.168.20.1
guest_ip=192.168.20.10
add_guest_vm_and_connections ovn-gw-2 $vrf_id           \
    $default_route_mac $default_route $default_route_gw \
    $guest_gw_ip $guest_ip

check multinode_nbctl --gateway-port $lrp_guest_join --add-route lr-nat-add \
    $lr_guest dnat_and_snat 172.16.10.2 192.168.10.10

OVS_WAIT_UNTIL([m_central_as ovn-sbctl list Advertised_Route | grep -q 172.16.10.2])
OVS_WAIT_UNTIL([m_as ovn-gw-1 ip netns exec frr-ns ip route | grep -q 'ext1'])
OVS_WAIT_UNTIL([m_as ovn-gw-1 ip netns exec frr-ns ping -W 1 -c 1 172.16.10.2])
OVS_WAIT_UNTIL([m_as ovn-gw-2 ip netns exec frr-ns ip route | grep -q 'ext1'])
OVS_WAIT_UNTIL([m_as ovn-gw-2 ip netns exec frr-ns ping -W 1 -c 1 172.16.10.2])

AT_CLEANUP

AT_SETUP([HA: Check for missing garp on leader when BFD goes back up])
# Network topology
#    ┌────────────────────────────────────────────────────────────────────────────────────────────────────────┐
#    │                                                                                                        │
#    │    ┌───────────────────┐    ┌───────────────────┐    ┌───────────────────┐    ┌───────────────────┐    │
#    │    │   ovn-chassis-1   │    │  ovn-gw-1         │    │  ovn-gw-2         │    │  ovn-chassis-2    │    │
#    │    └─────────┬─────────┘    └───────────────────┘    └───────────────────┘    └───────────────────┘    │
#    │    ┌─────────┴─────────┐                                                                               │
#    │    │       inside1     │                                                                               │
#    │    │   192.168.1.1/24  │                                                                               │
#    │    └─────────┬─────────┘                                                                               │
#    │    ┌─────────┴─────────┐                                                                               │
#    │    │       inside      │                                                                               │
#    │    └─────────┬─────────┘                                                                               │
#    │    ┌─────────┴─────────┐                                                                               │
#    │    │    192.168.1.254  │                                                                               │
#    │    │         R1        │                                                                               │
#    │    │    192.168.0.254  │                                                                               │
#    │    └─────────┬─────────┘                                                                               │
#    │              └------eth1---------------┬--------eth1-----------┐                                       │
#    │                             ┌──────────┴────────┐    ┌─────────┴─────────┐                             │
#    │                             │    192.168.1.254  │    │   192.168.1.254   │                             │
#    │                             │         R1        │    │         R1        │                             │
#    │                             │    192.168.0.254  │    │   192.168.0.254   │                             │
#    │                             └─────────┬─────────┘    └─────────┬─────────┘                             │
#    │                                       │                        │              ┌───────────────────┐    │
#    │                             ┌─────────┴─────────┐    ┌─────────┴─────────┐    │    192.168.0.1    │    │
#    │                             │       outside     │    │       outside     │    │        ext1       │    │
#    │                             └─────────┬─────────┘    └─────────┬─────────┘    └─────────┬─────────┘    │
#    │                             ┌─────────┴─────────┐    ┌─────────┴─────────┐    ┌─────────┴─────────┐    │
#    │                             │    ln-outside     │    │    ln-outside     │    │       ln-ext1     │    │
#    │                             └─────────┬─────────┘    └─────────┬─────────┘    └─────────┬─────────┘    │
#    │                             ┌─────────┴─────────┐    ┌─────────┴─────────┐    ┌─────────┴─────────┐    │
#    │                             │       br-ex       │    │       br-ex       │    │       br-ex       │    │
#    │                             └─────────┬─────────┘    └─────────┬─────────┘    └─────────┬─────────┘    │
#    │                                       └---------eth2-----------┴-------eth2-------------┘              │
#    │                                                                                                        │
#    └────────────────────────────────────────────────────────────────────────────────────────────────────────┘

# The goal of this test is the check that GARP are properly generated by higest priority traffic when
# BFD goes down, and back up, and this whether the BFD event is due either to some bfd packet lost
# or by gw death.
# gw1 is the highest priority gw; gw2 the second priority and gw3 is configured as the lowest priority gw.
# So gw3 should in this test neither send garp or receive packets.
#
# Enable vconn so we can check the GARP from a log perspective.
m_as ovn-gw-1 ovn-appctl vlog/set vconn:dbg
m_as ovn-gw-2 ovn-appctl vlog/set vconn:dbg
m_as ovn-gw-3 ovn-appctl vlog/set vconn:dbg
m_as ovn-gw-1 ovn-appctl vlog/disable-rate-limit
m_as ovn-gw-2 ovn-appctl vlog/disable-rate-limit
m_as ovn-gw-3 ovn-appctl vlog/disable-rate-limit

check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

ip_ch1=$(m_as ovn-chassis-1 ip a show dev eth1 | grep "inet " | awk '{print $2}'| cut -d '/' -f1)
ip_gw1=$(m_as ovn-gw-1 ip a show dev eth1 | grep "inet " | awk '{print $2}'| cut -d '/' -f1)
ip_gw2=$(m_as ovn-gw-2 ip a show dev eth1 | grep "inet " | awk '{print $2}'| cut -d '/' -f1)
ip_gw3=$(m_as ovn-gw-3 ip a show dev eth1 | grep "inet " | awk '{print $2}'| cut -d '/' -f1)

from_gw1_to_gw2=$(m_as ovn-gw-1 ovs-vsctl --bare --columns=name find interface options:remote_ip=$ip_gw2)
from_gw1_to_gw3=$(m_as ovn-gw-1 ovs-vsctl --bare --columns=name find interface options:remote_ip=$ip_gw3)
from_gw1_to_ch1=$(m_as ovn-gw-1 ovs-vsctl --bare --columns=name find interface options:remote_ip=$ip_ch1)
from_gw2_to_gw1=$(m_as ovn-gw-2 ovs-vsctl --bare --columns=name find interface options:remote_ip=$ip_gw1)
from_gw2_to_gw3=$(m_as ovn-gw-2 ovs-vsctl --bare --columns=name find interface options:remote_ip=$ip_gw3)
from_gw2_to_ch1=$(m_as ovn-gw-2 ovs-vsctl --bare --columns=name find interface options:remote_ip=$ip_ch1)
from_ch1_to_gw1=$(m_as ovn-chassis-1 ovs-vsctl --bare --columns=name find interface options:remote_ip=$ip_gw1)
from_ch1_to_gw2=$(m_as ovn-chassis-1 ovs-vsctl --bare --columns=name find interface options:remote_ip=$ip_gw2)

m_as ovn-chassis-1 ip link del hv1-vif1-p
m_as ovn-chassis-2 ip link del ext1-p

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-1 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-2 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-gw-3 ip link show | grep -q genev_sys])

check multinode_nbctl ls-add inside
check multinode_nbctl ls-add outside
check multinode_nbctl ls-add ext
check multinode_nbctl lsp-add inside inside1 -- lsp-set-addresses inside1 "f0:00:c0:a8:01:01 192.168.1.1"
check multinode_nbctl lsp-add ext ext1 -- lsp-set-addresses ext1 "00:00:c0:a8:00:01 192.168.0.1"

multinode_nbctl create Logical_Router name=R1
check multinode_nbctl lrp-add R1 R1_inside f0:00:c0:a8:01:fe 192.168.1.254/24
check multinode_nbctl -- lsp-add inside inside_R1 \
                      -- set Logical_Switch_Port inside_R1 type=router options:router-port=R1_inside \
                      -- lsp-set-addresses inside_R1 router

check multinode_nbctl lrp-add R1 R1_outside f0:00:c0:a8:00:fe 192.168.0.254/24
check multinode_nbctl -- lsp-add outside outside_R1 \
                      -- set Logical_Switch_Port outside_R1 type=router options:router-port=R1_outside \
                      -- lsp-set-addresses outside_R1 router

multinode_nbctl -- --id=@gc0 create Gateway_Chassis name=outside_gw1 chassis_name=ovn-gw-1 priority=20 \
                -- --id=@gc1 create Gateway_Chassis name=outside_gw2 chassis_name=ovn-gw-2 priority=10 \
                -- --id=@gc2 create Gateway_Chassis name=outside_gw3 chassis_name=ovn-gw-3 priority=5 \
                -- set Logical_Router_Port R1_outside 'gateway_chassis=[@gc0,@gc1,@gc2]'

# Create localnet port in outside
check multinode_nbctl lsp-add outside ln-outside
check multinode_nbctl lsp-set-addresses ln-outside unknown
check multinode_nbctl lsp-set-type ln-outside localnet
check multinode_nbctl lsp-set-options ln-outside network_name=public

# Create localnet port in ext1
check multinode_nbctl lsp-add ext ln-ext1
check multinode_nbctl lsp-set-addresses ln-ext1 unknown
check multinode_nbctl lsp-set-type ln-ext1 localnet
check multinode_nbctl lsp-set-options ln-ext1 network_name=public

# Make sure garp-max-timeout-sec is not set
m_as ovn-gw-1 ovs-vsctl remove open . external_ids garp-max-timeout-sec
m_as ovn-gw-2 ovs-vsctl remove open . external_ids garp-max-timeout-sec
m_as ovn-gw-3 ovs-vsctl remove open . external_ids garp-max-timeout-sec

m_as ovn-chassis-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-gw-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-gw-3 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex

m_as ovn-chassis-1 /data/create_fake_vm.sh inside1 hv1-vif1 f0:00:c0:a8:01:01 1500 192.168.1.1 24 192.168.1.254 2000::1/64 2000::a
m_as ovn-chassis-2 /data/create_fake_vm.sh ext1 ext1 00:00:c0:a8:00:01 1500 192.168.0.1 24 192.168.0.254 1000::3/64 1000::a

# There should be one ha_chassis_group with the name "R1_outside"
m_check_row_count HA_Chassis_Group 1 name=R1_outside

# There should be 2 ha_chassis rows in SB DB.
m_check_row_count HA_Chassis 3 'chassis!=[[]]'

ha_ch=$(m_fetch_column HA_Chassis_Group ha_chassis)
m_check_column "$ha_ch" HA_Chassis _uuid

gw1_chassis=$(m_fetch_column Chassis _uuid name=ovn-gw-1)
gw2_chassis=$(m_fetch_column Chassis _uuid name=ovn-gw-2)
gw3_chassis=$(m_fetch_column Chassis _uuid name=ovn-gw-3)

wait_bfd_enabled() {
    chassis=$1
    interface=$2
    echo "Using $chassis $interface"
    OVS_WAIT_UNTIL([test 1 = $(m_as $chassis ovs-vsctl --bare --columns bfd find Interface name=$interface | \
grep "enable=true" | wc -l)
])
}

wait_bfd_up() {
    hv1=$1
    hv2=$2

    echo "checking bfd_status for $hv1 => $hv2"
    OVS_WAIT_UNTIL([
        state=$(m_as $hv1 ovs-vsctl get interface $hv2 bfd_status:state)
        remote_state=$(m_as $hv1 ovs-vsctl get interface $hv2 bfd_status:remote_state)
        echo "$(date +%H:%M:%S.%03N) bfd state = $state, remote_state = $remote_state"
        test "$state" = "up" -a "$remote_state" = "up"
    ])
}

# check BFD enablement on tunnel ports from ovn-gw-1 ##########
for chassis in $from_gw1_to_gw2 $from_gw1_to_gw3 $from_gw1_to_ch1; do
    echo "checking ovn-gw-1 -> $chassis"
    wait_bfd_enabled ovn-gw-1 $chassis
done

# check BFD enablement on tunnel ports from ovn-gw-2 ##########
for chassis in $from_gw2_to_gw1 $from_gw2_to_gw3 $from_gw2_to_ch1; do
    echo "checking ovn-gw-2 -> $chassis"
    wait_bfd_enabled ovn-gw-2 $chassis
done

# check BFD enablement on tunnel ports from ovn-chassis-1 ###########
for chassis in $from_ch1_to_gw1 $from_ch1_to_gw2; do
    echo "checking ovn-chassis-1 -> $chassis"
    wait_bfd_enabled ovn-chassis-1 $chassis
done

# Make sure there is no nft table left. Do not use nft directly as might not be installed in container.
gw1_pid=$(podman inspect -f '{{.State.Pid}}' ovn-gw-1)
nsenter --net=/proc/$gw1_pid/ns/net nft list tables | grep ovn-test && nsenter --net=/proc/$gw1_pid/ns/net nft delete table ip ovn-test
on_exit "nsenter --net=/proc/$gw1_pid/ns/net nft list tables | grep ovn-test && nsenter --net=/proc/$gw1_pid/ns/net nft delete table ip ovn-test"

for chassis in $from_gw1_to_gw2 $from_gw1_to_gw3 $from_gw1_to_ch1; do
    wait_bfd_up ovn-gw-1 $chassis
done
for chassis in $from_gw2_to_gw1 $from_gw2_to_gw3 $from_gw2_to_ch1; do
    wait_bfd_up ovn-gw-2 $chassis
done
for chassis in $from_ch1_to_gw1 $from_ch1_to_gw2; do
    wait_bfd_up ovn-chassis-1 $chassis
done

m_wait_row_count Port_Binding 1 logical_port=cr-R1_outside chassis=$gw1_chassis
check multinode_nbctl --wait=hv sync

start_tcpdump() {
    echo "$(date +%H:%M:%S.%03N) Starting tcpdump"
    M_START_TCPDUMP([ovn-chassis-1], [-neei hv1-vif1-p], [ch1])
    M_START_TCPDUMP([ovn-chassis-2], [-neei eth2], [ch2])
    M_START_TCPDUMP([ovn-gw-1], [-neei eth2], [gw1])
    M_START_TCPDUMP([ovn-gw-1], [-neei eth2 -Q out], [gw1_out])
    M_START_TCPDUMP([ovn-gw-2], [-neei eth2], [gw2])
    M_START_TCPDUMP([ovn-gw-2], [-neei eth2 -Q out], [gw2_out])
    M_START_TCPDUMP([ovn-gw-3], [-neei eth2], [gw3])
    M_START_TCPDUMP([ovn-gw-3], [-neei eth2 -Q out], [gw3_out])
}

stop_tcpdump() {
    echo "$(date +%H:%M:%S.%03N) Stopping tcpdump"
    m_kill 'ovn-gw-1 ovn-gw-2 ovn-gw-3 ovn-chassis-1 ovn-chassis-2' tcpdump
}

# Send packets from chassis2 (ext1) to chassis1
send_background_packets() {
    echo "$(date +%H:%M:%S.%03N) Sending packets in Background"
    start_tcpdump
    M_NS_DAEMONIZE([ovn-chassis-2], [ext1], [ping -f -i 0.1 192.168.1.1], [ping.pid])
}

stop_sending_background_packets() {
    echo "$(date +%H:%M:%S.%03N) Stopping Background process"
    m_as ovn-chassis-1 ps -ef | grep -v grep | grep -q ping && \
        m_as ovn-chassis-1 echo "Stopping ping on ovn-chassis-1" && killall ping
    m_as ovn-chassis-2 ps -ef | grep -v grep | grep -q ping && \
        m_as ovn-chassis-2 echo "Stopping ping on ovn-chassis-2" && killall ping
    stop_tcpdump
}

check_for_new_garps() {
    hv=$1
    expecting_garp=$2
    n_new_garps=$(cat ${hv}_out.tcpdump | grep -c "f0:00:c0:a8:00:fe > Broadcast, ethertype ARP (0x0806), length 42: Request who-has 192.168.0.254 tell 192.168.0.254, length 28")

    if [ "$expecting_garp" == "true" ]; then
        AS_BOX([$(date +%H:%M:%S.%03N) Waiting/checking for garp from $hv - Starting with $n_new_garps])
        OVS_WAIT_UNTIL([
            n_garps=$n_new_garps
            n_new_garps=$(cat ${hv}_out.tcpdump | grep -c "f0:00:c0:a8:00:fe > Broadcast, ethertype ARP (0x0806), length 42: Request who-has 192.168.0.254 tell 192.168.0.254, length 28")
            echo "We saw $n_new_garps so far on ${hv}."
            test "$n_garps" -ne "$n_new_garps"
        ])
    else
        AS_BOX([$(date +%H:%M:%S.%03N) Checking no garp from ${hv}])
        # Waiting a few seconds to get a chance to see unexpected garps.
        sleep 3
        n_garps=$(cat ${hv}_out.tcpdump | grep -c "f0:00:c0:a8:00:fe > Broadcast, ethertype ARP (0x0806), length 42: Request who-has 192.168.0.254 tell 192.168.0.254, length 28")
        AT_CHECK([test "$n_garps" -eq "$n_new_garps"])
    fi
}

check_for_new_echo_pkts() {
    hv=$1
    mac_src=$2
    mac_dst=$3
    expecting_pkts=$4
    n_new_echo_req=$(cat ${hv}.tcpdump | grep -c "$mac_src > $mac_dst, ethertype IPv4 (0x0800), length 98: 192.168.0.1 > 192.168.1.1: ICMP echo request")
    n_new_echo_rep=$(cat ${hv}.tcpdump | grep -c "$mac_dst > $mac_src, ethertype IPv4 (0x0800), length 98: 192.168.1.1 > 192.168.0.1: ICMP echo reply")

    if [ "$expecting_pkts" == "true" ]; then
        AS_BOX([$(date +%H:%M:%S.%03N) Waiting/checking for echo pkts through ${hv}])
        echo "Starting with $n_new_echo_req requests and $n_new_echo_rep replies so far on ${hv}."
        OVS_WAIT_UNTIL([
            n_echo_req=$n_new_echo_req
            n_echo_rep=$n_new_echo_rep
            n_new_echo_req=$(cat ${hv}.tcpdump | grep -c "$mac_src > $mac_dst, ethertype IPv4 (0x0800), length 98: 192.168.0.1 > 192.168.1.1: ICMP echo request")
            n_new_echo_rep=$(cat ${hv}.tcpdump | grep -c "$mac_dst > $mac_src, ethertype IPv4 (0x0800), length 98: 192.168.1.1 > 192.168.0.1: ICMP echo reply")
            echo "We saw $n_new_echo_req requests and $n_new_echo_rep replies so far on ${hv}."
            test "$n_echo_req" -ne "$n_new_echo_req" && test "$n_echo_rep" -ne "$n_new_echo_rep"
        ])
    else
        AS_BOX([$(date +%H:%M:%S.%03N) Checking no pkts from ${hv}])
        # Waiting a few seconds to get a chance to see unexpected pkts.
        sleep 3
        n_echo_req=$(cat ${hv}.tcpdump | grep -c "$mac_src > $mac_dst, ethertype IPv4 (0x0800), length 98: 192.168.0.1 > 192.168.1.1: ICMP echo request")
        n_echo_rep=$(cat ${hv}.tcpdump | grep -c "$mac_dst > $mac_src, ethertype IPv4 (0x0800), length 98: 192.168.1.1 > 192.168.0.1: ICMP echo reply")
        echo "We saw $n_new_echo_req requests and $n_new_echo_rep replies on ${hv}."
        AT_CHECK([test "$n_echo_req" -eq "$n_new_echo_req" && test "$n_echo_rep" -eq "$n_new_echo_rep"])
    fi
}

dump_statistics() {
    n1=$(m_as ovn-gw-1 grep -c Changing /var/log/ovn/ovn-controller.log)
    n2=$(m_as ovn-gw-2 grep -c Changing /var/log/ovn/ovn-controller.log)
    n3=$(m_as ovn-gw-3 grep -c Changing /var/log/ovn/ovn-controller.log)
    ch1_req=$(grep -c "ICMP echo request" ch1.tcpdump)
    ch1_rep=$(grep -c "ICMP echo reply" ch1.tcpdump)
    ch2_req=$(grep -c "ICMP echo request" ch2.tcpdump)
    ch2_rep=$(grep -c "ICMP echo reply" ch2.tcpdump)
    gw1_req=$(grep -c "ICMP echo request" gw1.tcpdump)
    gw1_rep=$(grep -c "ICMP echo reply" gw1.tcpdump)
    gw2_req=$(grep -c "ICMP echo request" gw2.tcpdump)
    gw2_rep=$(grep -c "ICMP echo reply" gw2.tcpdump)
    gw3_req=$(grep -c "ICMP echo request" gw3.tcpdump)
    gw3_rep=$(grep -c "ICMP echo reply" gw3.tcpdump)
    echo "$n1 claims in gw1, $n2 in gw2 and $n3 on gw3"
    echo "ch2_request=$ch2_req gw1_request=$gw1_req gw2_request=$gw2_req gw3_request=$gw3_req ch1_request=$ch1_req ch1_reply=$ch1_rep gw1_reply=$gw1_rep gw2_reply=$gw2_rep gw3_reply=$gw3_rep ch2_reply=$ch2_rep"
}

check_migration_between_gw1_and_gw2() {
    action=$1
    send_background_packets

    # We make sure gw1 is leader since enough time that it generated all its garps.
    AS_BOX([$(date +%H:%M:%S.%03N) Waiting all garps sent by gw1])
    n_new_garps=$(cat gw1_out.tcpdump | grep -c "f0:00:c0:a8:00:fe > Broadcast, ethertype ARP (0x0806), length 42: Request who-has 192.168.0.254 tell 192.168.0.254, length 28")
    OVS_WAIT_UNTIL([
        n_garps=$n_new_garps
        echo "We saw $n_garps so far."
        # Garp delay might be up to 8 seconds.
        sleep 10
        n_new_garps=$(cat gw1_out.tcpdump | grep -c "f0:00:c0:a8:00:fe > Broadcast, ethertype ARP (0x0806), length 42: Request who-has 192.168.0.254 tell 192.168.0.254, length 28")
        test "$n_garps" -eq "$n_new_garps"
    ])

    # All packets should go through gw1, and none through gw2 or gw3.
    check_for_new_echo_pkts gw1 "00:00:c0:a8:00:01" "f0:00:c0:a8:00:fe" "true"
    check_for_new_echo_pkts gw2 "00:00:c0:a8:00:01" "f0:00:c0:a8:00:fe" "false"
    check_for_new_echo_pkts gw3 "00:00:c0:a8:00:01" "f0:00:c0:a8:00:fe" "false"

    flap_count_gw_1=$(m_as ovn-gw-1 ovs-vsctl get interface $from_gw1_to_gw2 bfd_status | sed 's/.*flap_count=\"\([[0-9]]*\).*/\1/g')
    flap_count_gw_2=$(m_as ovn-gw-2 ovs-vsctl get interface $from_gw2_to_gw1 bfd_status | sed 's/.*flap_count=\"\([[0-9]]*\).*/\1/g')

    if [ test "$action" == "stop_bfd" ]; then
        AS_BOX([$(date +%H:%M:%S.%03N) Blocking bfd on gw1 (from $ip_gw1 to $ip_gw2)])
        nsenter --net=/proc/$gw1_pid/ns/net nft add table ip ovn-test
        nsenter --net=/proc/$gw1_pid/ns/net nft 'add chain ip ovn-test INPUT { type filter hook input priority 0; policy accept; }'
        # Drop BFD from gw-1 to gw-2: geneve port (6081), inner port 3784 (0xec8), Session state Up, Init, Down.
        nsenter --net=/proc/$gw1_pid/ns/net nft add rule ip ovn-test INPUT ip daddr $ip_gw1 ip saddr $ip_gw2 udp dport 6081 '@th,416,16 == 0x0ec8 @th,472,8 == 0xc0  counter drop'
        nsenter --net=/proc/$gw1_pid/ns/net nft add rule ip ovn-test INPUT ip daddr $ip_gw1 ip saddr $ip_gw2 udp dport 6081 '@th,416,16 == 0x0ec8 @th,472,8 == 0x80  counter drop'
        nsenter --net=/proc/$gw1_pid/ns/net nft add rule ip ovn-test INPUT ip daddr $ip_gw1 ip saddr $ip_gw2 udp dport 6081 '@th,416,16 == 0x0ec8 @th,472,8 == 0x40  counter drop'

        # We do not check that packets go through gw2 as BFD between chassis-2 and gw1 is still up
    fi

    if [ test "$action" == "kill_gw2" ]; then
        AS_BOX([$(date +%H:%M:%S.%03N) Killing gw2 ovn-controller])
        on_exit 'm_as ovn-gw-2 /usr/share/openvswitch/scripts/ovs-ctl status ||
                 m_as ovn-gw-2 /usr/share/openvswitch/scripts/ovs-ctl start --system-id=ovn-gw-1'
        on_exit 'm_as ovn-gw-2 /usr/share/ovn/scripts/ovn-ctl status_controller ||
                 m_as ovn-gw-2 /usr/share/ovn/scripts/ovn-ctl start_controller ${CONTROLLER_SSL_ARGS}'

        m_as ovn-gw-2 kill -9 $(m_as ovn-gw-2 cat /run/ovn/ovn-controller.pid)
        m_as ovn-gw-2 kill -9 $(m_as ovn-gw-2 cat /run/openvswitch/ovs-vswitchd.pid)
        m_as ovn-gw-2 kill -9 $(m_as ovn-gw-2 cat /run/openvswitch/ovsdb-server.pid)
        # Also delete datapath (flows)
        m_as ovn-gw-2 ovs-dpctl del-dp system@ovs-system
    fi

    if [ test "$action" == "kill_gw1" ]; then
        AS_BOX([$(date +%H:%M:%S.%03N) Killing gw1 ovn-controller])
        on_exit 'm_as ovn-gw-1 /usr/share/openvswitch/scripts/ovs-ctl status ||
                 m_as ovn-gw-1 /usr/share/openvswitch/scripts/ovs-ctl start --system-id=ovn-gw-1'
        on_exit 'm_as ovn-gw-1 /usr/share/ovn/scripts/ovn-ctl status_controller ||
                 m_as ovn-gw-1 /usr/share/ovn/scripts/ovn-ctl start_controller ${CONTROLLER_SSL_ARGS}'

        m_as ovn-gw-1 kill -9 $(m_as ovn-gw-1 cat /run/ovn/ovn-controller.pid)
        m_as ovn-gw-1 kill -9 $(m_as ovn-gw-1 cat /run/openvswitch/ovs-vswitchd.pid)
        m_as ovn-gw-1 kill -9 $(m_as ovn-gw-1 cat /run/openvswitch/ovsdb-server.pid)
        # Also delete datapath (flows)
        m_as ovn-gw-1 ovs-dpctl del-dp system@ovs-system
    fi

    if [ test "$action" == "kill_gw2" ]; then
        AS_BOX([$(date +%H:%M:%S.%03N) Waiting for flap count between gw1 and gw2 to increase])
        OVS_WAIT_UNTIL([
            new_flap_count=$(m_as ovn-gw-1 ovs-vsctl get interfac $from_gw1_to_gw2 bfd_status | sed 's/.*flap_count=\"\([[0-9]]*\).*/\1/g')
            echo "Comparing $new_flap_count versus $flap_count_gw_1"
            test "$new_flap_count" -gt "$((flap_count_gw_1))"
        ])
    else
        AS_BOX([$(date +%H:%M:%S.%03N) Waiting for flap count between gw2 and gw1 to increase])
        OVS_WAIT_UNTIL([
            new_flap_count=$(m_as ovn-gw-2 ovs-vsctl get interfac $from_gw2_to_gw1 bfd_status | sed 's/.*flap_count=\"\([[0-9]]*\).*/\1/g')
            echo "Comparing $new_flap_count versus $flap_count_gw_2"
            test "$new_flap_count" -gt "$((flap_count_gw_2))"
        ])

    fi
    AS_BOX([$(date +%H:%M:%S.%03N) Flapped!])

    # Wait a few more second for the fight.
    sleep 2
    AS_BOX([$(date +%H:%M:%S.%03N) Statistics after flapping])
    dump_statistics

    if [ test "$action" == "stop_bfd" ]; then
        # gw1 still alive and gw2 tried to claim => gw1 should restart generating garps.
        check_for_new_garps gw1 "true"
        check_for_new_garps gw2 "false"
        check_for_new_garps gw3 "false"
        check_for_new_echo_pkts gw1 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "true"
        check_for_new_echo_pkts gw2 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "false"
        check_for_new_echo_pkts gw3 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "false"
        check_for_new_echo_pkts ch1 f0:00:c0:a8:01:fe f0:00:c0:a8:01:01 "true"
        AS_BOX([$(date +%H:%M:%S.%03N) Unblocking bfd on gw1])
        nsenter --net=/proc/$gw1_pid/ns/net nft -a list ruleset
        nsenter --net=/proc/$gw1_pid/ns/net nft delete table ip ovn-test
    fi

    if [ test "$action" == "kill_gw2" ]; then
        # gw1 still alive, but gw2 did not try to claim => gw1 should not generate new garps.
        check_for_new_garps gw1 "false"
        check_for_new_garps gw2 "false"
        check_for_new_garps gw3 "false"
        check_for_new_echo_pkts gw1 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "true"
        check_for_new_echo_pkts gw2 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "false"
        check_for_new_echo_pkts gw3 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "false"
        check_for_new_echo_pkts ch1 f0:00:c0:a8:01:fe f0:00:c0:a8:01:01 "true"
        AS_BOX([$(date +%H:%M:%S.%03N) Restarting gw2 ovn-vswitchd])
        m_as ovn-gw-2 /usr/share/openvswitch/scripts/ovs-ctl start --system-id=ovn-gw-2

        AS_BOX([$(date +%H:%M:%S.%03N) Restarting gw2 ovn-controller])
        m_as ovn-gw-2 /usr/share/ovn/scripts/ovn-ctl start_controller ${CONTROLLER_SSL_ARGS}
    fi

    if [ test "$action" == "kill_gw1" ]; then
        # gw1 died => gw2 should generate garps.
        check_for_new_garps gw1 "false"
        check_for_new_garps gw2 "true"
        check_for_new_garps gw3 "false"
        check_for_new_echo_pkts gw1 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "false"
        check_for_new_echo_pkts gw2 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "true"
        check_for_new_echo_pkts gw3 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "false"
        check_for_new_echo_pkts ch1 f0:00:c0:a8:01:fe f0:00:c0:a8:01:01 "true"
        AS_BOX([$(date +%H:%M:%S.%03N) Restarting gw1 ovn-vswitchd])
        m_as ovn-gw-1 /usr/share/openvswitch/scripts/ovs-ctl start --system-id=ovn-gw-1

        AS_BOX([$(date +%H:%M:%S.%03N) Restarting gw1 ovn-controller])
        m_as ovn-gw-1 /usr/share/ovn/scripts/ovn-ctl start_controller ${CONTROLLER_SSL_ARGS}
    fi

    # The network is now restored => packets should go through gw1 and reach chassis-1.
    check_for_new_echo_pkts gw1 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "true"
    check_for_new_echo_pkts gw2 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "false"
    check_for_new_echo_pkts gw3 00:00:c0:a8:00:01 f0:00:c0:a8:00:fe "false"
    check_for_new_echo_pkts ch1 f0:00:c0:a8:01:fe f0:00:c0:a8:01:01 "true"
    AS_BOX([$(date +%H:%M:%S.%03N) Statistics after network restored])
    dump_statistics
    stop_sending_background_packets
}

start_tcpdump
AS_BOX([$(date +%H:%M:%S.%03N) Sending packet from hv1-vif1(inside1) to ext1])
M_NS_CHECK_EXEC([ovn-chassis-1], [hv1-vif1], [ping -c3 -q -i 0.1 192.168.0.1 | FORMAT_PING],
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
stop_tcpdump

# It should have gone through gw1 and not gw2
AS_BOX([$(date +%H:%M:%S.%03N) Checking it went through gw1 and not gw2])
AT_CHECK([cat gw2.tcpdump | grep "ICMP echo"], [1], [dnl
])

AT_CHECK([cat gw1.tcpdump | grep "ICMP echo" | cut -d  ' ' -f2-15], [0], [dnl
f0:00:c0:a8:00:fe > 00:00:c0:a8:00:01, ethertype IPv4 (0x0800), length 98: 192.168.1.1 > 192.168.0.1: ICMP echo request,
00:00:c0:a8:00:01 > f0:00:c0:a8:00:fe, ethertype IPv4 (0x0800), length 98: 192.168.0.1 > 192.168.1.1: ICMP echo reply,
f0:00:c0:a8:00:fe > 00:00:c0:a8:00:01, ethertype IPv4 (0x0800), length 98: 192.168.1.1 > 192.168.0.1: ICMP echo request,
00:00:c0:a8:00:01 > f0:00:c0:a8:00:fe, ethertype IPv4 (0x0800), length 98: 192.168.0.1 > 192.168.1.1: ICMP echo reply,
f0:00:c0:a8:00:fe > 00:00:c0:a8:00:01, ethertype IPv4 (0x0800), length 98: 192.168.1.1 > 192.168.0.1: ICMP echo request,
00:00:c0:a8:00:01 > f0:00:c0:a8:00:fe, ethertype IPv4 (0x0800), length 98: 192.168.0.1 > 192.168.1.1: ICMP echo reply,
])

# We stop bfd between gw1 & gw2, but keep gw1 & gw2 running.
check_migration_between_gw1_and_gw2 "stop_bfd"

# We simulate death of gw2. It should not have any effect.
check_migration_between_gw1_and_gw2 "kill_gw2"

# We simulate death of gw1. gw2 should take over.
check_migration_between_gw1_and_gw2 "kill_gw1"

AT_CLEANUP
])

AT_SETUP([ovn multinode bgp L2 EVPN])
check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

CHECK_VRF()

vni=10
ext_bgp_ip_gw1=42.42.$vni.11
host_bgp_ip_gw1=42.42.$vni.12
ext_bgp_ip_gw2=42.42.$vni.21
host_bgp_ip_gw2=42.42.$vni.22

# Create an flat, distributed OVN localnet switch, with EVPN configured.
 check m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
 check m_as ovn-gw-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
 check m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-evpn-local-ip=$host_bgp_ip_gw1
 check m_as ovn-gw-2 ovs-vsctl set open . external-ids:ovn-evpn-local-ip=$host_bgp_ip_gw2
 # The tunnels need to be created before the call to "m_setup_host_frr_router()".
 check m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-evpn-vxlan-ports=4789
 check m_as ovn-gw-2 ovs-vsctl set open . external-ids:ovn-evpn-vxlan-ports=4789

m_setup_external_frr_router ovn-gw-1 4200000100 $ext_bgp_ip_gw1 $ext_bgp_ip_gw1/24 $vni
m_setup_host_frr_router     ovn-gw-1 4210000000 $host_bgp_ip_gw1 $host_bgp_ip_gw1/24 $vni

m_setup_external_frr_router ovn-gw-2 4200000200 $ext_bgp_ip_gw2 $ext_bgp_ip_gw2/24 $vni
m_setup_host_frr_router     ovn-gw-2 4210000000 $host_bgp_ip_gw2 $host_bgp_ip_gw2/24 $vni

OVS_WAIT_UNTIL([m_as ovn-gw-1 vtysh -c 'show bgp neighbors' | grep -qE 'Connections established 1'])
OVS_WAIT_UNTIL([m_as ovn-gw-2 vtysh -c 'show bgp neighbors' | grep -qE 'Connections established 1'])

check multinode_nbctl ls-add ls                        \
    -- lsp-add ls ls-ln -- lsp-set-type ls-ln localnet \
    -- lsp-set-addresses ls-ln unknown                 \
    -- lsp-set-options ls-ln network_name=public

# Configure "workloads" (VIF LSPs) on both chassis.
check multinode_nbctl lsp-add ls w1                       \
    -- lsp-set-addresses w1 "00:00:00:00:00:01 10.0.0.11"
check multinode_nbctl lsp-add ls w2                       \
    -- lsp-set-addresses w2 "00:00:00:00:00:02 10.0.0.12"

check m_as ovn-gw-1 /data/create_fake_vm.sh w1 w1 00:00:00:00:00:01 1500 10.0.0.11 24 10.0.0.1 1000::11/64 1000::1
check m_as ovn-gw-2 /data/create_fake_vm.sh w2 w2 00:00:00:00:00:02 1500 10.0.0.12 24 10.0.0.1 1000::12/64 1000::1
m_wait_for_ports_up

# Enable EVPN support for the distributed logical switch and redistribute
# local FDBs.
check multinode_nbctl set logical_switch ls       \
    other_config:dynamic-routing-vni=$vni         \
    other_config:dynamic-routing-redistribute=fdb
check multinode_nbctl --wait=hv sync

OVS_WAIT_UNTIL([m_as ovn-gw-1 bridge fdb | grep vxlan-10 | grep -q "00:00:00:00:00:00"])
OVS_WAIT_UNTIL([m_as ovn-gw-2 bridge fdb | grep vxlan-10 | grep -q "00:00:00:00:00:00"])

AS_BOX([Checking Remote VTEPs learned in OVN])
OVS_WAIT_FOR_OUTPUT_UNQUOTED([m_as ovn-gw-1 ovn-appctl evpn/remote-vtep-list | sort], [0], [dnl
IP: $ext_bgp_ip_gw1, port: 4789, vni: 10
])
OVS_WAIT_FOR_OUTPUT_UNQUOTED([m_as ovn-gw-2 ovn-appctl evpn/remote-vtep-list | sort], [0], [dnl
IP: $ext_bgp_ip_gw2, port: 4789, vni: 10
])

AS_BOX([Check traffic to "fabric" hosts - simulate external workloads])
check m_as ovn-gw-1 ip link add evpn_host type veth peer evpn_host_peer
on_exit "m_as ovn-gw-1 ip link del evpn_host"
check m_as ovn-gw-1 ip link set netns frr-ns evpn_host_peer
check m_as ovn-gw-1 ip netns exec frr-ns ip link set evpn_host_peer master br-10
check m_as ovn-gw-1 ip netns exec frr-ns ip link set evpn_host_peer up
check m_as ovn-gw-1 ip link set evpn_host addr 00:00:00:00:01:00
check m_as ovn-gw-1 ip addr add dev evpn_host 10.0.0.41/24
check m_as ovn-gw-1 ip link set evpn_host up

check m_as ovn-gw-2 ip link add evpn_host type veth peer evpn_host_peer
on_exit "m_as ovn-gw-2 ip link del evpn_host"
check m_as ovn-gw-2 ip link set netns frr-ns evpn_host_peer
check m_as ovn-gw-2 ip netns exec frr-ns ip link set evpn_host_peer master br-10
check m_as ovn-gw-2 ip netns exec frr-ns ip link set evpn_host_peer up
check m_as ovn-gw-2 ip link set evpn_host addr 00:00:00:00:02:00
check m_as ovn-gw-2 ip addr add dev evpn_host 10.0.0.42/24
check m_as ovn-gw-2 ip link set evpn_host up

AS_BOX([Checking EVPN MACs on External BGP host])
OVS_WAIT_FOR_OUTPUT([m_as ovn-gw-1 ip netns exec frr-ns vtysh --vty_socket /run/frr/frr-ns -c 'show evpn mac vni all'], [0], [dnl

VNI 10 #MACs (local and remote) 2

Flags: N=sync-neighs, I=local-inactive, P=peer-active, X=peer-proxy
MAC               Type   Flags Intf/Remote ES/VTEP            VLAN  Seq #'s
00:00:00:00:00:01 remote       42.42.10.12                          0/0
00:00:00:00:01:00 local        evpn_host_peer                       0/0
])

OVS_WAIT_FOR_OUTPUT([m_as ovn-gw-2 ip netns exec frr-ns vtysh --vty_socket /run/frr/frr-ns -c 'show evpn mac vni all'], [0], [dnl

VNI 10 #MACs (local and remote) 2

Flags: N=sync-neighs, I=local-inactive, P=peer-active, X=peer-proxy
MAC               Type   Flags Intf/Remote ES/VTEP            VLAN  Seq #'s
00:00:00:00:00:02 remote       42.42.10.22                          0/0
00:00:00:00:02:00 local        evpn_host_peer                       0/0
])

AS_BOX([Check traffic to "fabric" hosts - ping from fabric])
OVS_WAIT_UNTIL([m_as ovn-gw-1 ping -W 1 -c 1 10.0.0.11])
OVS_WAIT_UNTIL([m_as ovn-gw-2 ping -W 1 -c 1 10.0.0.12])

# Remove "workloads" (VIF LSPs) on both chassis.
check multinode_nbctl --wait=hv lsp-del w1 -- lsp-del w2

AS_BOX([Checking EVPN MACs on External BGP host])
OVS_WAIT_FOR_OUTPUT([m_as ovn-gw-1 ip netns exec frr-ns vtysh --vty_socket /run/frr/frr-ns -c 'show evpn mac vni all'], [0], [dnl

VNI 10 #MACs (local and remote) 1

Flags: N=sync-neighs, I=local-inactive, P=peer-active, X=peer-proxy
MAC               Type   Flags Intf/Remote ES/VTEP            VLAN  Seq #'s
00:00:00:00:01:00 local        evpn_host_peer                       0/0
])

OVS_WAIT_FOR_OUTPUT([m_as ovn-gw-2 ip netns exec frr-ns vtysh --vty_socket /run/frr/frr-ns -c 'show evpn mac vni all'], [0], [dnl

VNI 10 #MACs (local and remote) 1

Flags: N=sync-neighs, I=local-inactive, P=peer-active, X=peer-proxy
MAC               Type   Flags Intf/Remote ES/VTEP            VLAN  Seq #'s
00:00:00:00:02:00 local        evpn_host_peer                       0/0
])

AT_CLEANUP

AT_SETUP([redirect-bridged to non-gw destination switch port])

check_fake_multinode_setup
cleanup_multinode_resources
# This test uses the following logical network:
#
# +-----------+
# | ls-public |
# +-----------+
#       |
#       |
# +-----------+
# | gw-router |
# +-----------+
#       |
#       |
# +-----------+
# | ls-local  |
# +-----------+
#
# The router port from gw-router to ls-public is a distributed gateway port.
# It also has options:redirect-type=bridged set.
#
# We create vm1 attached to ls-local that is bound to ovn-chassis-1. We
# also create vm2 attached to ls-public that is bound to ovn-chassis-1.
# The DGP on gw-router is bound to ovn-gw-1.
#
# Our goal is to successfully ping from vm1 to vm2. In order for this to
# work, the ping will have to traverse gw-router. vm1 and vm2 are bound to
# the same chassis, but the DGP is bound to ovn-gw-1. We therefore expect
# the following:
#
# The ping starts by entering ls-local on ovn-chassis-1.
# The ping then goes through the ingress pipeline of gw-router on ovn-chassis-1.
# The ping then goes through ls-public's localnet port to reach ovn-gw-1.
# The ping's destination MAC should be the DGP's MAC. So the ping will
# get processed first by ls-public on ovn-gw-1, then will be redirected to
# gw-router on ovn-gw-1. The packet will then re-enter ls-public on ovn-gw-1.
# The ping will then get redirected over the localnet back to ovn-chassis-1.
# From here, the ls-public pipeline can run and the ping will be output to vm2.
#

check multinode_nbctl ls-add ls-local
check multinode_nbctl lsp-add ls-local vm1
check multinode_nbctl lsp-set-addresses vm1 "00:00:00:00:01:02 10.0.0.2 10::2"

check multinode_nbctl ls-add ls-public
check multinode_nbctl lsp-add ls-public vm2
check multinode_nbctl lsp-set-addresses vm2 "00:00:00:00:02:02 20.0.0.2 20::2"

check multinode_nbctl lsp-add ls-public ln-public
check multinode_nbctl lsp-set-type ln-public localnet
check multinode_nbctl lsp-set-addresses ln-public unknown
check multinode_nbctl lsp-set-options ln-public network_name=public

check multinode_nbctl lr-add gw-router

check multinode_nbctl lrp-add gw-router ro-local 00:00:00:00:01:01 10.0.0.1/8 10::1/64
check multinode_nbctl lsp-add ls-local local-ro
check multinode_nbctl lsp-set-type local-ro router
check multinode_nbctl lsp-set-addresses local-ro router
check multinode_nbctl lsp-set-options local-ro router-port=ro-local

check multinode_nbctl lrp-add gw-router ro-public 00:00:00:00:02:01 20.0.0.1/8 20::1/64
check multinode_nbctl lsp-add ls-public public-ro
check multinode_nbctl lsp-set-type public-ro router
check multinode_nbctl lsp-set-addresses public-ro router
check multinode_nbctl lsp-set-options public-ro router-port=ro-public

check multinode_nbctl lrp-set-gateway-chassis ro-public ovn-gw-1
check multinode_nbctl lrp-set-redirect-type ro-public bridged

m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-1 ovs-vsctl set open . external-ids:ovn-chassis-mac-mappings="public:aa:bb:cc:dd:01:01"
m_as ovn-gw-1 ovs-vsctl set open . external-ids:ovn-chassis-mac-mappings="public:aa:bb:cc:dd:02:01"

m_as ovn-chassis-1 /data/create_fake_vm.sh vm1 vm1 00:00:00:00:01:02 1342 10.0.0.2 8 10.0.0.1 10::2/64 10::1
m_as ovn-chassis-1 /data/create_fake_vm.sh vm2 vm2 00:00:00:00:02:02 1342 20.0.0.2 8 20.0.0.1 20::2/64 20::1

m_wait_for_ports_up

M_NS_CHECK_EXEC([ovn-chassis-1], [vm1], [ping -q -c 3 -i 0.3 -w 2 20.0.0.2 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])

AT_CLEANUP

AT_SETUP([IPv6 NA received on non resident chassis])
m_as ovn-chassis-1 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex
m_as ovn-chassis-2 ovs-vsctl set open . external-ids:ovn-bridge-mappings=public:br-ex

check_fake_multinode_setup

# Delete the multinode NB and OVS resources before starting the test.
cleanup_multinode_resources

m_as ovn-chassis-1 ip link del ls1p1-p
m_as ovn-chassis-1 ip link del ls1p2-p
m_as ovn-chassis-1 ip link del ls2p1-p

OVS_WAIT_UNTIL([m_as ovn-chassis-1 ip link show | grep -q genev_sys])
OVS_WAIT_UNTIL([m_as ovn-chassis-2 ip link show | grep -q genev_sys])

check multinode_nbctl ls-add ls1
check multinode_nbctl lsp-add ls1 ls1p1
check multinode_nbctl lsp-set-addresses ls1p1 "00:00:00:01:01:02 192.168.1.1 2001::1"
check multinode_nbctl lsp-add ls1 ls1p2
check multinode_nbctl lsp-set-addresses ls1p2 "00:00:00:01:02:02 192.168.1.2 2001::2"
check multinode_nbctl lr-add lr1
check multinode_nbctl lrp-add lr1 lr1-ls1 00:00:00:00:00:01 192.168.1.254/24 2001::a/64
check multinode_nbctl lsp-add ls1 ls1-lr1
check multinode_nbctl lsp-set-addresses ls1-lr1 "00:00:00:00:00:01 192.168.1.254 2001::a"
check multinode_nbctl lsp-set-type ls1-lr1 router
check multinode_nbctl lsp-set-options ls1-lr1 router-port=lr1-ls1

check multinode_nbctl lrp-add lr1 lr1-ls2 00:00:00:00:00:02 192.168.2.254/24 2002::a/64
check multinode_nbctl ls-add ls2
check multinode_nbctl lsp-add ls2 ls2-lr1
check multinode_nbctl lsp-set-addresses ls2-lr1 "00:00:00:00:00:02 192.168.2.254 2002::a"
check multinode_nbctl lsp-set-type ls2-lr1 router
check multinode_nbctl lsp-set-options ls2-lr1 router-port=lr1-ls2
check multinode_nbctl lsp-add ls2 ls2p1
check multinode_nbctl lsp-set-addresses ls2p1 "00:00:00:02:01:02 192.168.2.1 2002::1"

check multinode_nbctl lsp-add ls1 ls1p3
check multinode_nbctl lsp-set-addresses ls1p3 "00:00:00:01:03:02 192.168.1.3 2001::3"
check multinode_nbctl lrp-add lr1 lr1-pub 0a:0a:56:33:02:ff 172.18.86.254/24 6812:86::254/64
check multinode_nbctl ls-add pub                    \
    -- lsp-add pub pub-lr1                          \
    -- lsp-set-type pub-lr1 router                  \
    -- lsp-set-addresses pub-lr1 router             \
    -- lsp-set-options pub-lr1 router-port=lr1-pub  \
    -- lsp-add pub pub-ln                           \
    -- lsp-set-type pub-ln localnet                 \
    -- lsp-set-addresses pub-ln unknown             \
    -- lsp-set-options pub-ln network_name=public

check multinode_nbctl lrp-set-gateway-chassis lr1-pub ovn-chassis-2

check multinode_nbctl lr-nat-add lr1 dnat_and_snat 172.18.86.11 192.168.1.1 ls1p1 0a:0a:56:33:02:11
check multinode_nbctl lr-nat-add lr1 dnat_and_snat 6812:86::11 2001::1 ls1p1 0a:0a:56:33:02:11

m_as ovn-chassis-1 /data/create_fake_vm.sh ls1p1 ls1p1 00:00:00:01:01:02 1500 192.168.1.1 24 192.168.1.254 2001::1/64 2001::a
m_as ovn-chassis-1 /data/create_fake_vm.sh ls1p2 ls1p2 00:00:00:01:02:02 1500 192.168.1.2 24 192.168.1.254 2001::2/64 2001::a
m_as ovn-chassis-1 /data/create_fake_vm.sh ls2p1 ls2p1 00:00:00:02:01:02 1500 192.168.2.1 24 192.168.2.254 2002::2/64 2002::a
m_add_internal_port ovn-chassis-1 ovn-ext1 br-ex ext1 172.18.86.101/24 "" 6812:86::101/64
m_add_internal_port ovn-chassis-2 ovn-ext2 br-ex ext2 172.18.86.102/24 "" 6812:86::102/64

m_central_as ovn-sbctl --all destroy  mac_binding

M_NS_CHECK_EXEC([ovn-chassis-1], [ls1p1], [ping -q -c 3 -i 0.3 -w 2 172.18.86.101 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
m_wait_row_count mac_binding 1 ip="172.18.86.101" logical_port="lr1-pub"

M_NS_CHECK_EXEC([ovn-chassis-1], [ls1p1], [ping -q -c 3 -i 0.3 -w 2 172.18.86.102 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
m_wait_row_count mac_binding 1 ip="172.18.86.102" logical_port="lr1-pub"

M_NS_CHECK_EXEC([ovn-chassis-1], [ls1p1], [ping6 -q -c 3 -i 0.3 -w 2 6812:86::101 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
m_wait_row_count mac_binding 1 ip="6812\:86\:\:101" logical_port="lr1-pub"

M_NS_CHECK_EXEC([ovn-chassis-1], [ls1p1], [ping6 -q -c 3 -i 0.3 -w 2 6812:86::102 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
m_wait_row_count mac_binding 1 ip="6812\:86\:\:102" logical_port="lr1-pub"

# We remove all mac_bindings to check that neighbor learning works also in the other direction.
m_central_as ovn-sbctl --all destroy  mac_binding

# Also remove addresses learned on ovn-ext
M_NS_CHECK_EXEC([ovn-chassis-1], [ovn-ext1], [ip -6 neigh del 6812:86::11 dev ext1 lladdr 0a:0a:56:33:02:11])
M_NS_CHECK_EXEC([ovn-chassis-2], [ovn-ext2], [ip -6 neigh del 6812:86::11 dev ext2 lladdr 0a:0a:56:33:02:11])

M_NS_CHECK_EXEC([ovn-chassis-1], [ovn-ext1], [ping -q -c 3 -i 0.3 -w 2 172.18.86.11 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
m_wait_row_count mac_binding 1 ip="172.18.86.101" logical_port="lr1-pub"

M_NS_CHECK_EXEC([ovn-chassis-2], [ovn-ext2], [ping -q -c 3 -i 0.3 -w 2 172.18.86.11 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
m_wait_row_count mac_binding 1 ip="172.18.86.101" logical_port="lr1-pub"

M_NS_CHECK_EXEC([ovn-chassis-1], [ovn-ext1], [ping6 -q -c 3 -i 0.3 -w 2 6812:86::11 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
m_wait_row_count mac_binding 1 ip="6812\:86\:\:101" logical_port="lr1-pub"

M_NS_CHECK_EXEC([ovn-chassis-2], [ovn-ext2], [ping6 -q -c 3 -i 0.3 -w 2 6812:86::11 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
m_wait_row_count mac_binding 1 ip="6812\:86\:\:102" logical_port="lr1-pub"

# Ping from ext2 while no mac_binding yet.
m_central_as ovn-sbctl --all destroy  mac_binding
M_NS_CHECK_EXEC([ovn-chassis-2], [ovn-ext2], [ip -6 neigh del 6812:86::11 dev ext2 lladdr 0a:0a:56:33:02:11])

M_NS_CHECK_EXEC([ovn-chassis-2], [ovn-ext2], [ping6 -q -c 3 -i 0.3 -w 2 6812:86::11 | FORMAT_PING], \
[0], [dnl
3 packets transmitted, 3 received, 0% packet loss, time 0ms
])
m_wait_row_count mac_binding 1 ip="6812\:86\:\:102" logical_port="lr1-pub"

AT_CLEANUP
